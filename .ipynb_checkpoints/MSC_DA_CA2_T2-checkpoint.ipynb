{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217e0670",
   "metadata": {},
   "source": [
    "# Download libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "268f0605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytz in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (2022.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tweepy in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (4.12.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from requests<3,>=2.27.0->tweepy) (2022.12.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: statsmodels in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (0.13.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from statsmodels) (1.21.6)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from statsmodels) (21.3)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from statsmodels) (0.5.3)\n",
      "Requirement already satisfied: scipy>=1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from statsmodels) (1.7.3)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from statsmodels) (1.5.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=21.3->statsmodels) (3.0.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from pandas>=0.25->statsmodels) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.25->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: six in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: plotly in c:\\programdata\\anaconda3\\lib\\site-packages (5.6.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from plotly) (8.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from plotly) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytz\n",
    "!pip install tweepy\n",
    "!pip install statsmodels\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8bf5c8",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30ec967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import time\n",
    "from pytz import timezone\n",
    "import tweepy\n",
    "import json\n",
    "import statsmodels.api as sm\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "from pandas.errors import EmptyDataError\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85e1ba",
   "metadata": {},
   "source": [
    "# Loading the credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65039743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter API connection successful.\n"
     ]
    }
   ],
   "source": [
    "# Load the Twitter API credentials from the config file\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    consumer_key = config['consumer_key']\n",
    "    consumer_secret = config['consumer_secret']\n",
    "    access_token = config['access_token']\n",
    "    access_token_secret = config['access_token_secret']\n",
    "    \n",
    "# Verify the Twitter API credentials\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "try:\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "    user = api.verify_credentials()\n",
    "    print(\"Twitter API connection successful.\")\n",
    "except tweepy.error.TweepError as e:\n",
    "    print(\"Error: Failed to verify Twitter API credentials.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d1610",
   "metadata": {},
   "source": [
    "# Disabling warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "374410ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c567c",
   "metadata": {},
   "source": [
    "# Downloading and save twitter data\n",
    "The free tier of the twitter API holds the limitation of:</br>\n",
    "<b>**7 Day tweet history limit </br>\n",
    "**1500 tweet request limit </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a17cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tweets found for the date range: 2023-02-22 to 2023-02-23\n",
      "No tweets found for the date range: 2023-02-23 to 2023-02-24\n",
      "No tweets found for the date range: 2023-02-24 to 2023-02-25\n",
      "No tweets found for the date range: 2023-02-25 to 2023-02-26\n",
      "No tweets found for the date range: 2023-02-26 to 2023-02-27\n",
      "No tweets found for the date range: 2023-02-27 to 2023-02-28\n",
      "No tweets found for the date range: 2023-02-28 to 2023-03-01\n",
      "No tweets found for the date range: 2023-03-01 to 2023-03-02\n",
      "No tweets found for the date range: 2023-03-02 to 2023-03-03\n",
      "No tweets found for the date range: 2023-03-03 to 2023-03-04\n",
      "No tweets found for the date range: 2023-03-04 to 2023-03-05\n",
      "No tweets found for the date range: 2023-03-05 to 2023-03-06\n",
      "No tweets found for the date range: 2023-03-06 to 2023-03-07\n",
      "No tweets found for the date range: 2023-03-07 to 2023-03-08\n",
      "No tweets found for the date range: 2023-03-08 to 2023-03-09\n",
      "No tweets found for the date range: 2023-03-09 to 2023-03-10\n",
      "No tweets found for the date range: 2023-03-10 to 2023-03-11\n",
      "No tweets found for the date range: 2023-03-11 to 2023-03-12\n",
      "No tweets found for the date range: 2023-03-12 to 2023-03-13\n",
      "No tweets found for the date range: 2023-03-13 to 2023-03-14\n",
      "No tweets found for the date range: 2023-03-14 to 2023-03-15\n",
      "No tweets found for the date range: 2023-03-15 to 2023-03-16\n",
      "No tweets found for the date range: 2023-03-16 to 2023-03-17\n",
      "No tweets found for the date range: 2023-03-17 to 2023-03-18\n",
      "No tweets found for the date range: 2023-03-18 to 2023-03-19\n",
      "No tweets found for the date range: 2023-03-19 to 2023-03-20\n",
      "No tweets found for the date range: 2023-03-20 to 2023-03-21\n",
      "No tweets found for the date range: 2023-03-21 to 2023-03-22\n",
      "No tweets found for the date range: 2023-03-22 to 2023-03-23\n",
      "No tweets found for the date range: 2023-03-23 to 2023-03-24\n",
      "No tweets found for the date range: 2023-03-24 to 2023-03-25\n",
      "No tweets found for the date range: 2023-03-25 to 2023-03-26\n",
      "No tweets found for the date range: 2023-03-26 to 2023-03-27\n",
      "No tweets found for the date range: 2023-03-27 to 2023-03-28\n",
      "No tweets found for the date range: 2023-03-28 to 2023-03-29\n",
      "No tweets found for the date range: 2023-03-29 to 2023-03-30\n",
      "No tweets found for the date range: 2023-03-30 to 2023-03-31\n",
      "No tweets found for the date range: 2023-03-31 to 2023-04-01\n",
      "No tweets found for the date range: 2023-04-01 to 2023-04-02\n",
      "No tweets found for the date range: 2023-04-02 to 2023-04-03\n",
      "No tweets found for the date range: 2023-04-03 to 2023-04-04\n",
      "No tweets found for the date range: 2023-04-04 to 2023-04-05\n",
      "No tweets found for the date range: 2023-04-05 to 2023-04-06\n",
      "No tweets found for the date range: 2023-04-06 to 2023-04-07\n",
      "No tweets found for the date range: 2023-04-07 to 2023-04-08\n",
      "No tweets found for the date range: 2023-04-08 to 2023-04-09\n",
      "No tweets found for the date range: 2023-04-09 to 2023-04-10\n",
      "No tweets found for the date range: 2023-04-10 to 2023-04-11\n",
      "No tweets found for the date range: 2023-04-11 to 2023-04-12\n",
      "No tweets found for the date range: 2023-04-12 to 2023-04-13\n",
      "No tweets found for the date range: 2023-04-13 to 2023-04-14\n",
      "No tweets found for the date range: 2023-04-14 to 2023-04-15\n",
      "No tweets found for the date range: 2023-04-15 to 2023-04-16\n",
      "No tweets found for the date range: 2023-04-16 to 2023-04-17\n",
      "No tweets found for the date range: 2023-04-17 to 2023-04-18\n",
      "No tweets found for the date range: 2023-04-18 to 2023-04-19\n",
      "No tweets found for the date range: 2023-04-19 to 2023-04-20\n",
      "No tweets found for the date range: 2023-04-20 to 2023-04-21\n",
      "No tweets found for the date range: 2023-04-21 to 2023-04-22\n",
      "No tweets found for the date range: 2023-04-22 to 2023-04-23\n",
      "No tweets found for the date range: 2023-04-23 to 2023-04-24\n",
      "No tweets found for the date range: 2023-04-24 to 2023-04-25\n",
      "No tweets found for the date range: 2023-04-25 to 2023-04-26\n",
      "No tweets found for the date range: 2023-04-26 to 2023-04-27\n",
      "No tweets found for the date range: 2023-04-27 to 2023-04-28\n",
      "No tweets found for the date range: 2023-04-28 to 2023-04-29\n",
      "No tweets found for the date range: 2023-04-29 to 2023-04-30\n",
      "No tweets found for the date range: 2023-04-30 to 2023-05-01\n",
      "No tweets found for the date range: 2023-05-01 to 2023-05-02\n",
      "No tweets found for the date range: 2023-05-02 to 2023-05-03\n",
      "No tweets found for the date range: 2023-05-03 to 2023-05-04\n",
      "No tweets found for the date range: 2023-05-04 to 2023-05-05\n",
      "No tweets found for the date range: 2023-05-05 to 2023-05-06\n",
      "No tweets found for the date range: 2023-05-06 to 2023-05-07\n",
      "No tweets found for the date range: 2023-05-07 to 2023-05-08\n",
      "No tweets found for the date range: 2023-05-08 to 2023-05-09\n",
      "No tweets found for the date range: 2023-05-09 to 2023-05-10\n",
      "No tweets found for the date range: 2023-05-10 to 2023-05-11\n",
      "No tweets found for the date range: 2023-05-11 to 2023-05-12\n",
      "No tweets found for the date range: 2023-05-12 to 2023-05-13\n",
      "No tweets found for the date range: 2023-05-13 to 2023-05-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 632\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get today's date\n",
    "today = datetime.now().date()\n",
    "\n",
    "# Create a log file with today's date in the name\n",
    "log_file = f\"TwitterAPI_{today}.log\"\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO)\n",
    "\n",
    "# Define the topic and initial date range\n",
    "topic = \"(ios OR apple OR AAPL OR iphone OR ipad)\"\n",
    "start_date = today - timedelta(days=91)\n",
    "\n",
    "# Create a loop to run for 91 days\n",
    "for _ in range(91):\n",
    "    # Calculate the end date for the current iteration\n",
    "    end_date = start_date + timedelta(days=1)\n",
    "    \n",
    "    # Format the dates as strings\n",
    "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Define the search query with the current date range\n",
    "    query = f\"{topic} until:{end_date_str} since:{start_date_str}\"\n",
    "    \n",
    "    # Fetch tweets on the specified topic\n",
    "    try:\n",
    "        tweets = []\n",
    "        for tweet in tweepy.Cursor(api.search_tweets, q=query, lang='en', tweet_mode='extended').items(1500):\n",
    "            tweets.append({\n",
    "                'Date': tweet.created_at.date(),\n",
    "                'Tweet': tweet.full_text\n",
    "            })\n",
    "        \n",
    "        if len(tweets) > 0:\n",
    "            msg = \"Tweets downloaded successfully for the date range: {} to {}\"\n",
    "            logging.info(msg.format(start_date_str, end_date_str))\n",
    "            \n",
    "            # Convert the tweets list into a DataFrame\n",
    "            df_new = pd.DataFrame(tweets)\n",
    "            \n",
    "            # Check if the CSV file already exists\n",
    "            if os.path.isfile('tweets.csv'):\n",
    "                # Read the existing data from the CSV file\n",
    "                try:\n",
    "                    df_existing = pd.read_csv('tweets.csv')\n",
    "                    \n",
    "                    # Check if the existing DataFrame has any columns\n",
    "                    if df_existing.columns.empty:\n",
    "                        # Handle the case when the CSV file is empty\n",
    "                        df_existing = pd.DataFrame()\n",
    "                        \n",
    "                except EmptyDataError:\n",
    "                    # Handle the case when the CSV file is empty\n",
    "                    df_existing = pd.DataFrame()\n",
    "                \n",
    "                # Check if the existing DataFrame is empty\n",
    "                if df_existing.empty:\n",
    "                    # Save the new DataFrame to a new CSV file\n",
    "                    df_new.to_csv('tweets.csv', index=False)\n",
    "                    logging.info(\"New CSV file created with the downloaded tweets.\")\n",
    "                else:\n",
    "                    # Concatenate the existing and new data\n",
    "                    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "                    \n",
    "                    # Save the combined DataFrame to the CSV file\n",
    "                    df_combined.to_csv('tweets.csv', index=False)\n",
    "                    logging.info(\"Tweets appended to the existing CSV file.\")\n",
    "            else:\n",
    "                # Save the new DataFrame to a new CSV file\n",
    "                df_new.to_csv('tweets.csv', index=False)\n",
    "                logging.info(\"New CSV file created with the downloaded tweets.\")\n",
    "        else:\n",
    "            msg = \"No tweets found for the date range: {} to {}\"\n",
    "            logging.info(msg.format(start_date_str, end_date_str))\n",
    "            \n",
    "    except tweepy.TweepyException as e:\n",
    "        if e.api_code == 88:\n",
    "            # Rate limit reached, wait for the specified duration\n",
    "            wait_time = int(e.response.headers['Retry-After'])\n",
    "            msg = \"Rate limit reached. Sleeping for: {} seconds.\"\n",
    "            logging.info(msg.format(wait_time))\n",
    "            time.sleep(wait_time)\n",
    "        logging.error(\"Error: Failed to download tweets.\")\n",
    "        logging.error(e)\n",
    "    \n",
    "    # Update the start date for the next iteration\n",
    "    start_date = end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b9608",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e34aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "# Load the tweets from the CSV file\n",
    "df = pd.read_csv('tweets.csv')\n",
    "\n",
    "# Perform sentiment analysis using TextBlob\n",
    "df['sentiment'] = df['Tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Classify sentiment as positive, negative, or neutral\n",
    "df['sentiment_label'] = df['sentiment'].apply(lambda x: 'Positive' if x > 0 else 'Negative' if x < 0 else 'Neutral')\n",
    "\n",
    "# Save the updated DataFrame to CSV\n",
    "df.to_csv('tweets_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ad8d6",
   "metadata": {},
   "source": [
    "# Time series forecast of the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3693114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get today's date\n",
    "today = datetime.now().date()\n",
    "\n",
    "# Define the topic and initial date range\n",
    "topic = \"(ios OR apple OR AAPL OR iphone OR ipad)\"\n",
    "start_date = today - timedelta(days=91)\n",
    "\n",
    "# Create a loop to run for 91 days\n",
    "for _ in range(91):\n",
    "    # Calculate the end date for the current iteration\n",
    "    end_date = start_date + timedelta(days=1)\n",
    "    \n",
    "    # Format the dates as strings\n",
    "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Define the search query with the current date range\n",
    "    query = f\"{topic} until:{end_date_str} since:{start_date_str}\"\n",
    "    \n",
    "    # Fetch tweets on the specified topic\n",
    "    try:\n",
    "        tweets = []\n",
    "        for tweet in tweepy.Cursor(api.search_tweets, q=query, lang='en', tweet_mode='extended').items(1500):\n",
    "            tweets.append({\n",
    "                'Date': tweet.created_at.date(),\n",
    "                'Tweet': tweet.full_text\n",
    "            })\n",
    "        \n",
    "        if len(tweets) > 0:\n",
    "            print(\"Tweets downloaded successfully for the date range:\", start_date_str, \"to\", end_date_str)\n",
    "            \n",
    "            # Convert the tweets list into a DataFrame\n",
    "            df_new = pd.DataFrame(tweets)\n",
    "            \n",
    "            # Check if the CSV file already exists\n",
    "            if os.path.isfile('tweets.csv'):\n",
    "                # Read the existing data from the CSV file\n",
    "                try:\n",
    "                    df_existing = pd.read_csv('tweets.csv')\n",
    "                except EmptyDataError:\n",
    "                    # Handle the case when the CSV file is empty\n",
    "                    df_existing = pd.DataFrame()\n",
    "                \n",
    "                # Check if the existing DataFrame is empty\n",
    "                if df_existing.empty:\n",
    "                    # Save the new DataFrame to a new CSV file\n",
    "                    df_new.to_csv('tweets.csv', index=False)\n",
    "                    print(\"New CSV file created with the downloaded tweets.\")\n",
    "                else:\n",
    "                    # Concatenate the existing and new data\n",
    "                    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "                    \n",
    "                    # Save the combined DataFrame to the CSV file\n",
    "                    df_combined.to_csv('tweets.csv', index=False)\n",
    "                    print(\"Tweets appended to the existing CSV file.\")\n",
    "            else:\n",
    "                # Save the new DataFrame to a new CSV file\n",
    "                df_new.to_csv('tweets.csv', index=False)\n",
    "                print(\"New CSV file created with the downloaded tweets.\")\n",
    "        else:\n",
    "            print(\"No tweets found for the date range:\", start_date_str, \"to\", end_date_str)\n",
    "            \n",
    "    except tweepy.TweepyException as e:\n",
    "        if e.api_code == 88:\n",
    "            # Rate limit reached, wait for the specified duration\n",
    "            wait_time = int(e.response.headers['Retry-After'])\n",
    "            print(\"Rate limit reached. Sleeping for:\", wait_time, \"seconds.\")\n",
    "            time.sleep(wait_time)\n",
    "        print(\"Error: Failed to download tweets.\")\n",
    "        print(e)\n",
    "    \n",
    "    # Update the start date for the next iteration\n",
    "    start_date = end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440bb85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert forecast data to strings\n",
    "forecast_1w_str = forecast_1w.to_string(header=False)\n",
    "forecast_1m_str = forecast_1m.to_string(header=False)\n",
    "forecast_3m_str = forecast_3m.to_string(header=False)\n",
    "# Print the forecast data\n",
    "print(\"1 Week Forecast:\")\n",
    "print(forecast_1w_str)\n",
    "print(\"1 Month Forecast:\")\n",
    "print(forecast_1m_str)\n",
    "print(\"3 Months Forecast:\")\n",
    "print(forecast_3m_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1901ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
