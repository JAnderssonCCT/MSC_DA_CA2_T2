{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf890e85",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a3125b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import time\n",
    "from pytz import timezone\n",
    "import tweepy\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85e1ba",
   "metadata": {},
   "source": [
    "# Loading the credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65039743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter API connection successful.\n"
     ]
    }
   ],
   "source": [
    "# Load the Twitter API credentials from the config file\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    consumer_key = config['consumer_key']\n",
    "    consumer_secret = config['consumer_secret']\n",
    "    access_token = config['access_token']\n",
    "    access_token_secret = config['access_token_secret']\n",
    "    \n",
    "# Verify the Twitter API credentials\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "try:\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "    user = api.verify_credentials()\n",
    "    print(\"Twitter API connection successful.\")\n",
    "except tweepy.error.TweepError as e:\n",
    "    print(\"Error: Failed to verify Twitter API credentials.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d1610",
   "metadata": {},
   "source": [
    "# Disabling warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "374410ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c567c",
   "metadata": {},
   "source": [
    "# Downloading twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba1a17cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the topic and date range\n",
    "topic = 'Ireland'\n",
    "end_date = datetime.datetime(2023, 5, 21, tzinfo=timezone('Europe/London'))\n",
    "start_date = datetime.datetime(2022, 5, 21, tzinfo=timezone('Europe/London'))\n",
    "\n",
    "# Fetch tweets on the specified topic\n",
    "try:\n",
    "    tweets = []\n",
    "    for tweet in tweepy.Cursor(api.search_tweets, q=topic, lang='en', tweet_mode='extended').items(2000):\n",
    "        tweets.append({\n",
    "            'Date': tweet.created_at.date(),\n",
    "            'Tweet': tweet.full_text\n",
    "        })\n",
    "    print(\"Tweets downloaded successfully.\")\n",
    "except tweepy.TweepyException as e:\n",
    "    if e.api_code == 88:\n",
    "        # Rate limit reached, wait for the specified duration\n",
    "        wait_time = int(e.response.headers['Retry-After'])\n",
    "        print(\"Rate limit reached. Sleeping for:\", wait_time, \"seconds.\")\n",
    "        time.sleep(wait_time)\n",
    "    print(\"Error: Failed to download tweets.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb131fa",
   "metadata": {},
   "source": [
    "# Saving twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0be7189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets appended to tweets.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert the tweets list into a DataFrame\n",
    "df_new = pd.DataFrame(tweets)\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "if os.path.isfile('tweets.csv'):\n",
    "    # Read the existing data from the CSV file\n",
    "    df_existing = pd.read_csv('tweets.csv')\n",
    "    \n",
    "    # Concatenate the existing and new data\n",
    "    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "    \n",
    "    # Save the combined DataFrame to the CSV file\n",
    "    df_combined.to_csv('tweets.csv', index=False)\n",
    "    print(\"Tweets appended to the existing CSV file.\")\n",
    "else:\n",
    "    # Save the new DataFrame to a new CSV file\n",
    "    df_new.to_csv('tweets.csv', index=False)\n",
    "    print(\"New CSV file created with the downloaded tweets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c79475",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0087f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "# Load the tweets from the CSV file\n",
    "df = pd.read_csv('tweets.csv')\n",
    "\n",
    "# Perform sentiment analysis using TextBlob\n",
    "df['sentiment'] = df['Tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Classify sentiment as positive, negative, or neutral\n",
    "df['sentiment_label'] = df['sentiment'].apply(lambda x: 'Positive' if x > 0 else 'Negative' if x < 0 else 'Neutral')\n",
    "\n",
    "# Save the updated DataFrame to CSV\n",
    "df.to_csv('tweets_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d4da4",
   "metadata": {},
   "source": [
    "# Time series forecast of the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8319bb8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load the tweets sentiment data from the CSV file\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweets_sentiment.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Resample the data to daily frequency\u001b[39;00m\n\u001b[0;32m     12\u001b[0m df_daily \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mresample(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Date'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from datetime import timedelta\n",
    "\n",
    "# Load the tweets sentiment data from the CSV file\n",
    "df = pd.read_csv('tweets_sentiment.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Resample the data to daily frequency\n",
    "df_daily = df.set_index('Date').resample('D').mean()\n",
    "\n",
    "# Fit an ARIMA model to the sentiment data\n",
    "model = ARIMA(df_daily['sentiment'], order=(1, 0, 0))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Generate predictions for the next 3 months\n",
    "forecast_start_date = df_daily.index[-1] + timedelta(days=1)\n",
    "forecast_end_date = forecast_start_date + timedelta(days=90)\n",
    "forecast = model_fit.get_forecast(steps=(forecast_end_date - forecast_start_date).days)\n",
    "forecast_mean = forecast.predicted_mean\n",
    "\n",
    "# Create the Plotly figure for the sentiment forecast\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_daily.index, y=df_daily['sentiment'], name='Actual Sentiment'))\n",
    "fig.add_trace(go.Scatter(x=forecast_mean.index, y=forecast_mean, name='Forecasted Sentiment'))\n",
    "\n",
    "# Customize the layout of the figure\n",
    "fig.update_layout(title='Sentiment Forecast',\n",
    "                  xaxis_title='Date',\n",
    "                  yaxis_title='Sentiment',\n",
    "                  legend_title='Sentiment Type')\n",
    "\n",
    "# Save the figure as an HTML file\n",
    "pio.write_html(fig, file='sentiment_forecast.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e54b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
