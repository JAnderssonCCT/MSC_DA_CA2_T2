{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217e0670",
   "metadata": {},
   "source": [
    "# Download libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f0605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pandas requests seaborn IPython flask requests_oauthlib pytz tweepy statsmodels plotly wordcloud textblob nltk requests_oauthlib flask pyspark dask textblob sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8bf5c8",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation and Analysis\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# File and Directory Operations\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Date and Time Handling\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from pytz import timezone\n",
    "\n",
    "# System-related\n",
    "import sys\n",
    "import socket\n",
    "\n",
    "# Twitter API\n",
    "import tweepy\n",
    "import json\n",
    "import requests_oauthlib\n",
    "\n",
    "# Plotting and Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Natural Language Processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "\n",
    "# Spark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "\n",
    "# Web Development (Flask)\n",
    "from flask import Flask, jsonify, request, render_template\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Other Dependencies\n",
    "import ast\n",
    "import re\n",
    "import warnings\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85e1ba",
   "metadata": {},
   "source": [
    "# Loading the credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65039743",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the Twitter API credentials from the config file\n",
    "    with open('config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "        consumer_key = config['consumer_key']\n",
    "        consumer_secret = config['consumer_secret']\n",
    "        access_token = config['access_token']\n",
    "        access_token_secret = config['access_token_secret']\n",
    "\n",
    "    # Verify the Twitter API credentials\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "    try:\n",
    "        api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "        user = api.verify_credentials()\n",
    "        logging.info(\"Twitter API connection successful.\")\n",
    "    except tweepy.error.TweepError as e:\n",
    "        logging.error(\"Error: Failed to verify Twitter API credentials.\")\n",
    "        logging.error(str(e))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Error: The config file 'config.json' does not exist.\")\n",
    "\n",
    "except json.JSONDecodeError as e:\n",
    "    logging.error(\"Error: Failed to load Twitter API credentials from 'config.json'.\")\n",
    "    logging.error(str(e))\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(\"An error occurred during a Twitter API connection.\")\n",
    "    logging.error(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d1610",
   "metadata": {},
   "source": [
    "# Disabling warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374410ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a90b5f",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f09131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get today's date\n",
    "today = datetime.now().date()\n",
    "\n",
    "# Create a log file with today's date in the name\n",
    "log_file = f\"TwitterAPI_{today}.log\"\n",
    "\n",
    "# Configure the root logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Create a FileHandler and set its formatter\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s','%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# Attach the FileHandler to the root logger\n",
    "logging.getLogger().addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c567c",
   "metadata": {},
   "source": [
    "# Downloading and save twitter data\n",
    "The free tier of the twitter API holds the limitation of:</br>\n",
    "<b>**10 Day tweet history limit </br>\n",
    "**1500 tweet request limit per 900sec circ. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a17cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the topic and initial date range\n",
    "topic = \"(ios OR apple OR AAPL OR iphone OR ipad)\"\n",
    "start_date = today - timedelta(days=91)\n",
    "\n",
    "# Create a loop to run for 91 days\n",
    "for _ in range(91):\n",
    "    # Calculate the end date for the current iteration\n",
    "    end_date = start_date + timedelta(days=1)\n",
    "    \n",
    "    # Format the dates as strings\n",
    "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Define the search query with the current date range\n",
    "    query = f\"{topic} until:{end_date_str} since:{start_date_str}\"\n",
    "    \n",
    "    # Fetch tweets on the specified topic\n",
    "    try:\n",
    "        tweets = []\n",
    "        for tweet in tweepy.Cursor(api.search_tweets, q=query, lang='en', tweet_mode='extended').items(3000):\n",
    "            tweets.append({\n",
    "                'Date': tweet.created_at.date(),\n",
    "                'Tweet': tweet.full_text\n",
    "            })\n",
    "        \n",
    "        if len(tweets) > 0:\n",
    "            msg = \"Tweets downloaded successfully for the date range: {} to {}\"\n",
    "            logging.info(msg.format(start_date_str, end_date_str))\n",
    "            \n",
    "            # Convert the tweets list into a DataFrame\n",
    "            df_new = pd.DataFrame(tweets)\n",
    "            \n",
    "            # Check if the CSV file already exists\n",
    "            if os.path.isfile('tweets.csv'):\n",
    "                # Read the existing data from the CSV file\n",
    "                try:\n",
    "                    df_existing = pd.read_csv('tweets.csv')\n",
    "                    \n",
    "                    # Check if the existing DataFrame has any columns\n",
    "                    if df_existing.columns.empty:\n",
    "                        # Handle the case when the CSV file is empty\n",
    "                        df_existing = pd.DataFrame()\n",
    "                        \n",
    "                except EmptyDataError:\n",
    "                    # Handle the case when the CSV file is empty\n",
    "                    df_existing = pd.DataFrame()\n",
    "                \n",
    "                # Check if the existing DataFrame is empty\n",
    "                if df_existing.empty:\n",
    "                    # Save the new DataFrame to a new CSV file\n",
    "                    df_new.to_csv('tweets.csv', index=False)\n",
    "                    logging.info(\"New CSV file created with the downloaded tweets.\")\n",
    "                else:\n",
    "                    # Concatenate the existing and new data\n",
    "                    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "                    \n",
    "                    # Save the combined DataFrame to the CSV file\n",
    "                    df_combined.to_csv('tweets.csv', index=False)\n",
    "                    logging.info(\"Tweets appended to the existing CSV file.\")\n",
    "            else:\n",
    "                # Save the new DataFrame to a new CSV file\n",
    "                df_new.to_csv('tweets.csv', index=False)\n",
    "                logging.info(\"New CSV file created with the downloaded tweets.\")\n",
    "        else:\n",
    "            msg = \"No tweets found for the date range: {} to {}\"\n",
    "            logging.info(msg.format(start_date_str, end_date_str))\n",
    "            \n",
    "    except tweepy.TweepyException as e:\n",
    "        if e.api_code == 88:\n",
    "            # Rate limit reached, wait for the specified duration\n",
    "            wait_time = int(e.response.headers['Retry-After'])\n",
    "            msg = \"Rate limit reached. Sleeping for: {} seconds.\"\n",
    "            logging.info(msg.format(wait_time))\n",
    "            time.sleep(wait_time)\n",
    "        logging.error(\"Error: Failed to download tweets.\")\n",
    "        logging.error(e)\n",
    "    \n",
    "    # Update the start date for the next iteration\n",
    "    start_date = end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f23fea",
   "metadata": {},
   "source": [
    "# Log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10531093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the log file into a DataFrame\n",
    "log_df = pd.read_csv(log_file, sep=\":\", names=[\"Timestamp\",\"Type\",\"Message\",\"Sleep\"])\n",
    "\n",
    "# Define the CSV file name for saving the log DataFrame\n",
    "csv_file = f\"TwitterAPI_{today}.csv\"\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "if os.path.isfile(csv_file):\n",
    "    # Append the log DataFrame to the existing CSV file\n",
    "    log_df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "else:\n",
    "    # Save the log DataFrame to a new CSV file\n",
    "    log_df.to_csv(csv_file, index=False)\n",
    "\n",
    "# Display the log DataFrame\n",
    "log_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482b2ac4",
   "metadata": {},
   "source": [
    "# Cleaning tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c079f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the tweets data from the CSV file\n",
    "    df_tweets = pd.read_csv('tweets.csv')\n",
    "\n",
    "    # Convert the 'Tweet' column to string type\n",
    "    df_tweets['Tweet'] = df_tweets['Tweet'].astype(str)\n",
    "\n",
    "    # Compile the regular expressions\n",
    "    remove_chars_regex = re.compile(r'[^\\w\\s]')\n",
    "    remove_links_regex = re.compile(r'http\\S+|www\\S+')\n",
    "    remove_usernames_regex = re.compile(r'@[^\\s]+')\n",
    "\n",
    "    # Function to clean a single tweet\n",
    "    def clean_tweet(tweet):\n",
    "        # Remove unnecessary characters and links\n",
    "        cleaned_tweet = remove_chars_regex.sub('', tweet)\n",
    "        cleaned_tweet = remove_links_regex.sub('', cleaned_tweet)\n",
    "\n",
    "        # Remove Twitter usernames\n",
    "        cleaned_tweet = remove_usernames_regex.sub('', cleaned_tweet)\n",
    "\n",
    "        # Remove non-English words\n",
    "        cleaned_words = []\n",
    "        english_words = set(words.words())\n",
    "        for word in cleaned_tweet.split():\n",
    "            if word.lower() in english_words:\n",
    "                cleaned_words.append(word)\n",
    "        cleaned_tweet = ' '.join(cleaned_words)\n",
    "\n",
    "        return cleaned_tweet\n",
    "\n",
    "    # Clean the tweets column using vectorization\n",
    "    df_tweets['Tweet'] = df_tweets['Tweet'].str.replace(remove_chars_regex, '')\n",
    "    df_tweets['Tweet'] = df_tweets['Tweet'].str.replace(remove_links_regex, '')\n",
    "    df_tweets['Tweet'] = df_tweets['Tweet'].str.replace(remove_usernames_regex, '')\n",
    "    df_tweets['Tweet'] = df_tweets['Tweet'].apply(clean_tweet)\n",
    "\n",
    "    # Remove rows with empty tweet values\n",
    "    df_tweets = df_tweets.dropna(subset=['Tweet'])\n",
    "\n",
    "    # Save the cleaned tweets back to the CSV file\n",
    "    df_tweets.to_csv('tweets.csv', index=False)\n",
    "\n",
    "    # Log the execution of the tweet cleaning process\n",
    "    logging.info(\"Tweet cleaning process completed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Log any exceptions or errors that occur\n",
    "    logging.error(\"Error occurred during tweet cleaning process.\")\n",
    "    logging.error(str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fabffb7",
   "metadata": {},
   "source": [
    "# Tweet EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tweets data from the CSV file\n",
    "df_tweets = pd.read_csv('tweets.csv')\n",
    "# Perform basic exploratory data analysis (EDA)\n",
    "print(\"Number of tweets:\", len(df_tweets))\n",
    "print(\"Columns:\", df_tweets.columns)\n",
    "print(\"Sample tweets:\")\n",
    "print(df_tweets.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2a42bf",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e9c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the tweets from the CSV file\n",
    "    df = pd.read_csv('tweets.csv')\n",
    "\n",
    "    # Convert 'Tweet' column to string type\n",
    "    df['Tweet'] = df['Tweet'].astype(str)\n",
    "\n",
    "    # Perform sentiment analysis using TextBlob\n",
    "    df['sentiment'] = df['Tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "    # Classify sentiment as positive, negative, or neutral\n",
    "    df['sentiment_label'] = df['sentiment'].apply(lambda x: 'Positive' if x > 0 else 'Negative' if x < 0 else 'Neutral')\n",
    "\n",
    "    # Save the updated DataFrame to CSV\n",
    "    df.to_csv('tweets_sentiment.csv', index=False)\n",
    "\n",
    "except pd.errors.EmptyDataError:\n",
    "    # Handle the case when the CSV file is empty\n",
    "    logging.error(\"Error: The CSV file is empty.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # Handle the case when the CSV file is not found\n",
    "    logging.error(\"Error: The CSV file 'tweets.csv' does not exist.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle any other exceptions that may occur\n",
    "    logging.error(\"An error occurred during sentiment analysis and classification.\")\n",
    "    logging.error(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b845368",
   "metadata": {},
   "source": [
    "# Most frequent words wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b8e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word cloud of most frequent words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Convert float values in 'Tweet' column to strings\n",
    "df_tweets['Tweet'] = df_tweets['Tweet'].astype(str)\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(stopwords=stop_words, background_color='white').generate(' '.join(df_tweets['Tweet']))\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Most Frequent Words in Tweets')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dafc507",
   "metadata": {},
   "source": [
    "# Time series forecast of the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tweet sentiment data from the CSV file\n",
    "df = pd.read_csv('tweets_sentiment.csv', parse_dates=['Date'])\n",
    "\n",
    "# Set the 'Date' column as the index\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_data = df.iloc[:train_size]\n",
    "test_data = df.iloc[train_size:]\n",
    "\n",
    "# Ensure the index of test_data is a datetime index and sorted\n",
    "test_data.index = pd.to_datetime(test_data.index)\n",
    "test_data = test_data.sort_index()\n",
    "\n",
    "# Define the model configurations\n",
    "arima_order = (1, 0, 1)\n",
    "sarimax_order = (1, 0, 1)\n",
    "\n",
    "# Initialize the models\n",
    "arima_model = ARIMA(train_data['sentiment'], order=arima_order).fit()\n",
    "sarimax_model = SARIMAX(train_data['sentiment'], order=sarimax_order).fit()\n",
    "\n",
    "# Generate predictions for the test set\n",
    "arima_predictions = arima_model.predict(start=0, end=len(test_data)-1)\n",
    "sarimax_predictions = sarimax_model.predict(start=0, end=len(test_data)-1)\n",
    "\n",
    "# Evaluate model performance\n",
    "arima_rmse = mean_squared_error(test_data['sentiment'], arima_predictions, squared=False)\n",
    "sarimax_rmse = mean_squared_error(test_data['sentiment'], sarimax_predictions, squared=False)\n",
    "\n",
    "# Create Plotly figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot actual sentiment data\n",
    "ax.plot(df.index, df['sentiment'], label='Actual')\n",
    "\n",
    "# Plot forecasted sentiment data\n",
    "ax.plot(test_data.index, arima_predictions, label=f'ARIMA Forecast (RMSE: {arima_rmse:.2f})')\n",
    "ax.plot(test_data.index, sarimax_predictions, label=f'SARIMAX Forecast (RMSE: {sarimax_rmse:.2f})')\n",
    "\n",
    "# Set plot labels and legend\n",
    "ax.set_title('Time Series Forecast of Sentiment')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Sentiment')\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f32f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 1-week, 1-month, and 3-month forecasts using the SARIMAX model\n",
    "forecast_1w = sarimax_model.get_forecast(steps=7)\n",
    "forecast_1m = sarimax_model.get_forecast(steps=30)\n",
    "forecast_3m = sarimax_model.get_forecast(steps=90)\n",
    "\n",
    "# Convert forecast data to strings\n",
    "forecast_1w_str = forecast_1w.predicted_mean.to_string(header=False)\n",
    "forecast_1m_str = forecast_1m.predicted_mean.to_string(header=False)\n",
    "forecast_3m_str = forecast_3m.predicted_mean.to_string(header=False)\n",
    "\n",
    "# Print the forecast data\n",
    "print(\"1 Week Forecast:\")\n",
    "print(forecast_1w_str)\n",
    "print(\"1 Month Forecast:\")\n",
    "print(forecast_1m_str)\n",
    "print(\"3 Months Forecast:\")\n",
    "print(forecast_3m_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aed7e6",
   "metadata": {},
   "source": [
    "# Sentiment distribution by sentiment category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d692e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d34df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tweets data from the CSV file\n",
    "df_tweets = pd.read_csv('tweets_sentiment.csv')\n",
    "\n",
    "# Perform basic exploratory data analysis (EDA)\n",
    "print(\"Number of tweets:\", len(df_tweets))\n",
    "print(\"Columns:\", df_tweets.columns)\n",
    "print(\"Sample tweets:\")\n",
    "print(df_tweets.head())\n",
    "\n",
    "# Visualize sentiment distribution by sentiment category\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='sentiment', hue='sentiment_label', data=df_tweets)\n",
    "plt.title('Sentiment Distribution by Category')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb920f7a",
   "metadata": {},
   "source": [
    "# Sentiment distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffc738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Read the tweets data from the CSV file\n",
    "df_tweets = pd.read_csv('tweets_sentiment.csv')\n",
    "\n",
    "# Perform basic exploratory data analysis (EDA)\n",
    "print(\"Number of tweets:\", len(df_tweets))\n",
    "print(\"Columns:\", df_tweets.columns)\n",
    "print(\"Sample tweets:\")\n",
    "print(df_tweets.head())\n",
    "\n",
    "# Create a bar chart for sentiment distribution by sentiment category\n",
    "fig = px.bar(df_tweets, x='sentiment', color='sentiment_label',\n",
    "             title='Sentiment Distribution by Category', labels={'sentiment': 'Sentiment', 'count': 'Count'},\n",
    "             barmode='stack')\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(xaxis={'title': 'Sentiment'}, yaxis={'title': 'Count'})\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bf65ae",
   "metadata": {},
   "source": [
    "# Sentiment distribution over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a1fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' column to datetime\n",
    "df_tweets['Date'] = pd.to_datetime(df_tweets['Date'])\n",
    "\n",
    "# Group by date and sentiment to calculate counts\n",
    "df_sentiment_counts = df_tweets.groupby(['Date', 'sentiment']).size().reset_index(name='Count')\n",
    "\n",
    "# Pivot the data to have sentiment types as columns\n",
    "df_sentiment_pivot = df_sentiment_counts.pivot(index='Date', columns='sentiment', values='Count')\n",
    "\n",
    "# Plot the sentiment distribution over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=df_sentiment_pivot, dashes=False)\n",
    "plt.title('Sentiment Distribution Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Sentiment')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f8908b",
   "metadata": {},
   "source": [
    "# Most frequent words by sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372696a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocess tweets by filling missing values\n",
    "df_tweets['Tweet'].fillna('', inplace=True)\n",
    "\n",
    "# Tokenize the tweets into words\n",
    "tokenized_words = [word.lower() for tweet in df_tweets['Tweet'] for word in word_tokenize(tweet)]\n",
    "\n",
    "# Calculate the frequency distribution of words\n",
    "freq_dist = FreqDist(tokenized_words)\n",
    "most_common = freq_dist.most_common(20)\n",
    "\n",
    "# Create a bar chart for the most frequent words by sentiment\n",
    "fig = go.Figure()\n",
    "for sentiment in df_tweets['sentiment_label'].unique():\n",
    "    sentiment_words = [word for word, count in most_common if word.lower() in tokenized_words and df_tweets.loc[df_tweets['sentiment_label'] == sentiment, 'Tweet'].str.contains(word, case=False).any()]\n",
    "    fig.add_trace(go.Bar(x=sentiment_words, y=[freq_dist[word.lower()] for word in sentiment_words], name=sentiment))\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(title='Most Frequent Words by Sentiment', xaxis={'title': 'Word'}, yaxis={'title': 'Count'})\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692c8a48",
   "metadata": {},
   "source": [
    "# PySpark tweet upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cbc9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the values below with your own Twitter API credentials\n",
    "ACCESS_TOKEN = access_token\n",
    "ACCESS_SECRET = access_token_secret\n",
    "CONSUMER_KEY = consumer_key\n",
    "CONSUMER_SECRET = consumer_secret\n",
    "\n",
    "# Create an OAuth1 authentication object\n",
    "my_auth = requests_oauthlib.OAuth1(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "def get_tweets():\n",
    "    # Set the URL and query parameters for filtering tweets\n",
    "    url = 'https://stream.twitter.com/1.1/statuses/filter.json'\n",
    "    query_data = [('language', 'en'), ('track', 'ios OR apple OR AAPL OR iphone OR ipad')]\n",
    "    query_url = url + '?' + '&'.join([str(t[0]) + '=' + str(t[1]) for t in query_data])\n",
    "    \n",
    "    # Send the request to the Twitter API with authentication\n",
    "    response = requests.get(query_url, auth=my_auth, stream=True)\n",
    "    print(query_url, response)\n",
    "    return response\n",
    "\n",
    "def send_tweets_to_spark(http_resp, tcp_connection):\n",
    "    # Iterate over the response lines from the Twitter API\n",
    "    for line in http_resp.iter_lines():\n",
    "        try:\n",
    "            # Parse each line as JSON\n",
    "            full_tweet = json.loads(line)\n",
    "            tweet_text = full_tweet['text']\n",
    "            \n",
    "            # Print the tweet text and send it to the TCP connection\n",
    "            print(\"Tweet Text: \" + tweet_text)\n",
    "            print(\"------------------------------------------\")\n",
    "            tcp_connection.send(tweet_text + '\\n')\n",
    "        except:\n",
    "            e = sys.exc_info()[0]\n",
    "            print(\"Error: %s\" % e)\n",
    "\n",
    "# Set up TCP connection parameters\n",
    "TCP_IP = \"localhost\"\n",
    "TCP_PORT = 9009\n",
    "conn = None\n",
    "\n",
    "# Create a TCP socket and bind it to the specified IP and port\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "s.bind((TCP_IP, TCP_PORT))\n",
    "s.listen(1)\n",
    "\n",
    "print(\"Waiting for TCP connection...\")\n",
    "conn, addr = s.accept()\n",
    "print(\"Connected... Starting to get tweets.\")\n",
    "\n",
    "# Get tweets from the Twitter API\n",
    "resp = get_tweets()\n",
    "\n",
    "# Send the tweets to the TCP connection\n",
    "send_tweets_to_spark(resp, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506fe0c6",
   "metadata": {},
   "source": [
    "# Configure Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94093506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark configuration\n",
    "conf = SparkConf().setAppName(\"TwitterStreamApp\")\n",
    "\n",
    "# Create Spark context with the above configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create the Streaming Context from the above Spark context with interval size 2 seconds\n",
    "ssc = StreamingContext(sc, 2)\n",
    "\n",
    "# Set a checkpoint to allow RDD recovery\n",
    "ssc.checkpoint(\"checkpoint_TwitterApp\")\n",
    "\n",
    "# Read data from port 9009\n",
    "dataStream = ssc.socketTextStream(\"localhost\", 9009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf13bd",
   "metadata": {},
   "source": [
    "# Filter Hastags and stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe84c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to aggregate tag counts\n",
    "def aggregate_tags_count(new_values, total_sum):\n",
    "    return sum(new_values) + (total_sum or 0)\n",
    "\n",
    "# Function to get Spark SQL context instance\n",
    "def get_sql_context_instance(spark_context):\n",
    "    if 'sqlContextSingletonInstance' not in globals():\n",
    "        globals()['sqlContextSingletonInstance'] = SparkSession.builder.getOrCreate()\n",
    "    return globals()['sqlContextSingletonInstance']\n",
    "\n",
    "# Function to process RDD\n",
    "def process_rdd(time, rdd):\n",
    "    print(\"----------- %s -----------\" % str(time))\n",
    "    try:\n",
    "        # Get Spark SQL singleton context from the current context\n",
    "        sql_context = get_sql_context_instance(rdd.context)\n",
    "        \n",
    "        # Convert the RDD to DataFrame\n",
    "        row_rdd = rdd.map(lambda w: Row(hashtag=w[0], hashtag_count=w[1]))\n",
    "        hashtags_df = sql_context.createDataFrame(row_rdd)\n",
    "        \n",
    "        # Create a temporary view for querying\n",
    "        hashtags_df.createOrReplaceTempView(\"hashtags\")\n",
    "        \n",
    "        # Get the top 10 hashtags using SQL\n",
    "        hashtag_counts_df = sql_context.sql(\"SELECT hashtag, hashtag_count FROM hashtags ORDER BY hashtag_count DESC LIMIT 10\")\n",
    "        hashtag_counts_df.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error: %s\" % str(e))\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"TwitterStreamApp\").getOrCreate()\n",
    "\n",
    "# Create StreamingContext from Spark session with interval size 2 seconds\n",
    "ssc = StreamingContext(spark.sparkContext, 2)\n",
    "\n",
    "# Set checkpoint directory for RDD recovery\n",
    "ssc.checkpoint(\"checkpoint_TwitterApp\")\n",
    "\n",
    "# Read data from port 9009\n",
    "dataStream = ssc.socketTextStream(\"localhost\", 9009)\n",
    "\n",
    "# Split each tweet into words\n",
    "words = dataStream.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Filter the words to get only hashtags, then map each hashtag to be a pair of (hashtag, 1)\n",
    "hashtags = words.filter(lambda w: '#' in w).map(lambda x: (x, 1))\n",
    "\n",
    "# Adding the count of each hashtag to its last count\n",
    "tags_totals = hashtags.updateStateByKey(aggregate_tags_count)\n",
    "\n",
    "# Do processing for each RDD generated in each interval\n",
    "tags_totals.foreachRDD(process_rdd)\n",
    "\n",
    "# Start the streaming computation\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the streaming to finish\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708c620f",
   "metadata": {},
   "source": [
    "# Stream to dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f47aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_df_to_dashboard(df):\n",
    "    try:\n",
    "        # Extract the hashtags from the DataFrame and convert them into an array\n",
    "        top_tags = [str(t.hashtag) for t in df.select(\"hashtag\").collect()]\n",
    "        \n",
    "        # Extract the counts from the DataFrame and convert them into an array\n",
    "        tags_count = [p.hashtag_count for p in df.select(\"hashtag_count\").collect()]\n",
    "        \n",
    "        # Initialize and send the data through REST API\n",
    "        url = 'http://localhost:5001/updateData'\n",
    "        request_data = {'label': str(top_tags), 'data': str(tags_count)}\n",
    "        response = requests.post(url, data=request_data)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"Data sent to the dashboard successfully.\")\n",
    "        else:\n",
    "            print(\"Failed to send data to the dashboard. Status code:\", response.status_code)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error occurred while sending data to the dashboard:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f236face",
   "metadata": {},
   "source": [
    "# Building the Flask app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d41648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "labels = []\n",
    "values = []\n",
    "\n",
    "@app.route(\"/\")\n",
    "def get_chart_page():\n",
    "    \"\"\"\n",
    "    Route handler for the main chart page.\n",
    "    Clears the labels and values and renders the chart.html template.\n",
    "    \"\"\"\n",
    "    global labels, values\n",
    "    labels = []\n",
    "    values = []\n",
    "    return render_template('chart.html', values=values, labels=labels)\n",
    "\n",
    "@app.route('/refreshData')\n",
    "def refresh_graph_data():\n",
    "    \"\"\"\n",
    "    Route handler to refresh the graph data.\n",
    "    Returns the current labels and values as a JSON response.\n",
    "    \"\"\"\n",
    "    global labels, values\n",
    "    print(\"Labels now: \" + str(labels))\n",
    "    print(\"Data now: \" + str(values))\n",
    "    return jsonify(sLabel=labels, sData=values)\n",
    "\n",
    "@app.route('/updateData', methods=['POST'])\n",
    "def update_data():\n",
    "    \"\"\"\n",
    "    Route handler to update the data.\n",
    "    Expects 'label' and 'data' fields in the POST request form.\n",
    "    Updates the labels and values based on the received data.\n",
    "    \"\"\"\n",
    "    global labels, values\n",
    "    try:\n",
    "        if not request.form or 'data' not in request.form or 'label' not in request.form:\n",
    "            raise ValueError(\"Invalid data received. Missing 'label' or 'data' field.\")\n",
    "\n",
    "        labels = ast.literal_eval(request.form['label'])\n",
    "        values = ast.literal_eval(request.form['data'])\n",
    "        print(\"Labels received: \" + str(labels))\n",
    "        print(\"Data received: \" + str(values))\n",
    "        return \"success\", 201\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error occurred while updating data:\", str(e))\n",
    "        return \"error\", 400\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='localhost', port=5001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc1f948",
   "metadata": {},
   "source": [
    "# View Flask app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ffea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the URL of your Flask app\n",
    "app_url = 'http://localhost:5001/'\n",
    "\n",
    "# Embed the Flask app in Jupyter Notebook\n",
    "IFrame(src=app_url, width='100%', height=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
