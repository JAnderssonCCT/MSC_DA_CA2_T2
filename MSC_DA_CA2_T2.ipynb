{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217e0670",
   "metadata": {},
   "source": [
    "# Download libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f0605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pytz\n",
    "!pip install tweepy\n",
    "!pip install statsmodels\n",
    "!pip install plotly\n",
    "!pip install wordcloud\n",
    "!pip install textblob\n",
    "!pip install nltk\n",
    "!pip install requests_oauthlib\n",
    "!pip install flask\n",
    "!pip install ast\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8bf5c8",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ec967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "from pytz import timezone\n",
    "import tweepy\n",
    "import json\n",
    "import statsmodels.api as sm\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "from pandas.errors import EmptyDataError\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "import socket\n",
    "import requests_oauthlib\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext\n",
    "import requests\n",
    "from flask import Flask,jsonify,request\n",
    "from flask import render_template\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85e1ba",
   "metadata": {},
   "source": [
    "# Loading the credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65039743",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the Twitter API credentials from the config file\n",
    "    with open('config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "        consumer_key = config['consumer_key']\n",
    "        consumer_secret = config['consumer_secret']\n",
    "        access_token = config['access_token']\n",
    "        access_token_secret = config['access_token_secret']\n",
    "\n",
    "    # Verify the Twitter API credentials\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "    try:\n",
    "        api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "        user = api.verify_credentials()\n",
    "        logging.info(\"Twitter API connection successful.\")\n",
    "    except tweepy.error.TweepError as e:\n",
    "        logging.error(\"Error: Failed to verify Twitter API credentials.\")\n",
    "        logging.error(str(e))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Error: The config file 'config.json' does not exist.\")\n",
    "\n",
    "except json.JSONDecodeError as e:\n",
    "    logging.error(\"Error: Failed to load Twitter API credentials from 'config.json'.\")\n",
    "    logging.error(str(e))\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(\"An error occurred during a Twitter API connection.\")\n",
    "    logging.error(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d1610",
   "metadata": {},
   "source": [
    "# Disabling warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "374410ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9aff7a",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1037fd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get today's date\n",
    "today = datetime.now().date()\n",
    "\n",
    "# Create a log file with today's date in the name\n",
    "log_file = f\"TwitterAPI_{today}.log\"\n",
    "\n",
    "# Configure the root logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Create a FileHandler and set its formatter\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s','%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# Attach the FileHandler to the root logger\n",
    "logging.getLogger().addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c567c",
   "metadata": {},
   "source": [
    "# Downloading and save twitter data\n",
    "The free tier of the twitter API holds the limitation of:</br>\n",
    "<b>**10 Day tweet history limit </br>\n",
    "**1500 tweet request limit per 900sec circ. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a17cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the topic and initial date range\n",
    "topic = \"(ios OR apple OR AAPL OR iphone OR ipad)\"\n",
    "start_date = today - timedelta(days=91)\n",
    "\n",
    "# Create a loop to run for 91 days\n",
    "for _ in range(91):\n",
    "    # Calculate the end date for the current iteration\n",
    "    end_date = start_date + timedelta(days=1)\n",
    "    \n",
    "    # Format the dates as strings\n",
    "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Define the search query with the current date range\n",
    "    query = f\"{topic} until:{end_date_str} since:{start_date_str}\"\n",
    "    \n",
    "    # Fetch tweets on the specified topic\n",
    "    try:\n",
    "        tweets = []\n",
    "        for tweet in tweepy.Cursor(api.search_tweets, q=query, lang='en', tweet_mode='extended').items(3000):\n",
    "            tweets.append({\n",
    "                'Date': tweet.created_at.date(),\n",
    "                'Tweet': tweet.full_text\n",
    "            })\n",
    "        \n",
    "        if len(tweets) > 0:\n",
    "            msg = \"Tweets downloaded successfully for the date range: {} to {}\"\n",
    "            logging.info(msg.format(start_date_str, end_date_str))\n",
    "            \n",
    "            # Convert the tweets list into a DataFrame\n",
    "            df_new = pd.DataFrame(tweets)\n",
    "            \n",
    "            # Check if the CSV file already exists\n",
    "            if os.path.isfile('tweets.csv'):\n",
    "                # Read the existing data from the CSV file\n",
    "                try:\n",
    "                    df_existing = pd.read_csv('tweets.csv')\n",
    "                    \n",
    "                    # Check if the existing DataFrame has any columns\n",
    "                    if df_existing.columns.empty:\n",
    "                        # Handle the case when the CSV file is empty\n",
    "                        df_existing = pd.DataFrame()\n",
    "                        \n",
    "                except EmptyDataError:\n",
    "                    # Handle the case when the CSV file is empty\n",
    "                    df_existing = pd.DataFrame()\n",
    "                \n",
    "                # Check if the existing DataFrame is empty\n",
    "                if df_existing.empty:\n",
    "                    # Save the new DataFrame to a new CSV file\n",
    "                    df_new.to_csv('tweets.csv', index=False)\n",
    "                    logging.info(\"New CSV file created with the downloaded tweets.\")\n",
    "                else:\n",
    "                    # Concatenate the existing and new data\n",
    "                    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "                    \n",
    "                    # Save the combined DataFrame to the CSV file\n",
    "                    df_combined.to_csv('tweets.csv', index=False)\n",
    "                    logging.info(\"Tweets appended to the existing CSV file.\")\n",
    "            else:\n",
    "                # Save the new DataFrame to a new CSV file\n",
    "                df_new.to_csv('tweets.csv', index=False)\n",
    "                logging.info(\"New CSV file created with the downloaded tweets.\")\n",
    "        else:\n",
    "            msg = \"No tweets found for the date range: {} to {}\"\n",
    "            logging.info(msg.format(start_date_str, end_date_str))\n",
    "            \n",
    "    except tweepy.TweepyException as e:\n",
    "        if e.api_code == 88:\n",
    "            # Rate limit reached, wait for the specified duration\n",
    "            wait_time = int(e.response.headers['Retry-After'])\n",
    "            msg = \"Rate limit reached. Sleeping for: {} seconds.\"\n",
    "            logging.info(msg.format(wait_time))\n",
    "            time.sleep(wait_time)\n",
    "        logging.error(\"Error: Failed to download tweets.\")\n",
    "        logging.error(e)\n",
    "    \n",
    "    # Update the start date for the next iteration\n",
    "    start_date = end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f23fea",
   "metadata": {},
   "source": [
    "# Log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10531093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Type</th>\n",
       "      <th>Message</th>\n",
       "      <th>Sleep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-05-24 19</td>\n",
       "      <td>45</td>\n",
       "      <td>01 - WARNING - Rate limit reached. Sleeping for</td>\n",
       "      <td>484.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-05-24 19</td>\n",
       "      <td>53</td>\n",
       "      <td>55 - WARNING - Rate limit reached. Sleeping for</td>\n",
       "      <td>851.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-05-24 20</td>\n",
       "      <td>09</td>\n",
       "      <td>06 - WARNING - Rate limit reached. Sleeping for</td>\n",
       "      <td>841.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-05-24 20</td>\n",
       "      <td>24</td>\n",
       "      <td>09 - WARNING - Rate limit reached. Sleeping for</td>\n",
       "      <td>839.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-24 20</td>\n",
       "      <td>39</td>\n",
       "      <td>10 - WARNING - Rate limit reached. Sleeping for</td>\n",
       "      <td>839.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-05-24 20</td>\n",
       "      <td>54</td>\n",
       "      <td>11 - WARNING - Rate limit reached. Sleeping for</td>\n",
       "      <td>839.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-05-24 21</td>\n",
       "      <td>09</td>\n",
       "      <td>12 - WARNING - Rate limit reached. Sleeping for</td>\n",
       "      <td>839.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-05-24 21</td>\n",
       "      <td>24</td>\n",
       "      <td>13 - WARNING - Rate limit reached. Sleeping for</td>\n",
       "      <td>839.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-05-24 21</td>\n",
       "      <td>39</td>\n",
       "      <td>15 - WARNING - Rate limit reached. Sleeping for</td>\n",
       "      <td>838.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-05-24 21</td>\n",
       "      <td>54</td>\n",
       "      <td>09 - ERROR - Error occurred during tweet clean...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-05-24 21</td>\n",
       "      <td>54</td>\n",
       "      <td>09 - ERROR - name 're' is not defined</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-05-24 22</td>\n",
       "      <td>09</td>\n",
       "      <td>39 - ERROR - Error occurred during tweet clean...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-05-24 22</td>\n",
       "      <td>09</td>\n",
       "      <td>39 - ERROR - name 're' is not defined</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-05-24 22</td>\n",
       "      <td>12</td>\n",
       "      <td>07 - ERROR - Error occurred during tweet clean...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-05-24 22</td>\n",
       "      <td>12</td>\n",
       "      <td>07 - ERROR - name 'words' is not defined</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-05-24 22</td>\n",
       "      <td>13</td>\n",
       "      <td>15 - ERROR - Error occurred during tweet clean...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023-05-24 22</td>\n",
       "      <td>13</td>\n",
       "      <td>15 - ERROR -</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>**********************************************...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Resource \u001b[93mwords\u001b[0m not found.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Please use the NLTK Downloader to obtain the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>\u001b[31m&gt;&gt;&gt; import nltk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>&gt;&gt;&gt; nltk.download('words')</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\u001b[0m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>For more information see</td>\n",
       "      <td>https</td>\n",
       "      <td>//www.nltk.org/data.html</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Attempted to load \u001b[93mcorpora/words\u001b[0m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Searched in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>- 'C</td>\n",
       "      <td>\\\\Users\\\\OnceU/nltk_data'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>- 'C</td>\n",
       "      <td>\\\\ProgramData\\\\Anaconda3\\\\nltk_data'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>- 'C</td>\n",
       "      <td>\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>- 'C</td>\n",
       "      <td>\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>- 'C</td>\n",
       "      <td>\\\\Users\\\\OnceU\\\\AppData\\\\Roaming\\\\nltk_data'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>- 'C</td>\n",
       "      <td>\\\\nltk_data'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>- 'D</td>\n",
       "      <td>\\\\nltk_data'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>- 'E</td>\n",
       "      <td>\\\\nltk_data'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>**********************************************...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Timestamp  \\\n",
       "0                                       2023-05-24 19   \n",
       "1                                       2023-05-24 19   \n",
       "2                                       2023-05-24 20   \n",
       "3                                       2023-05-24 20   \n",
       "4                                       2023-05-24 20   \n",
       "5                                       2023-05-24 20   \n",
       "6                                       2023-05-24 21   \n",
       "7                                       2023-05-24 21   \n",
       "8                                       2023-05-24 21   \n",
       "9                                       2023-05-24 21   \n",
       "10                                      2023-05-24 21   \n",
       "11                                      2023-05-24 22   \n",
       "12                                      2023-05-24 22   \n",
       "13                                      2023-05-24 22   \n",
       "14                                      2023-05-24 22   \n",
       "15                                      2023-05-24 22   \n",
       "16                                      2023-05-24 22   \n",
       "17  **********************************************...   \n",
       "18                 Resource \u001b[93mwords\u001b[0m not found.   \n",
       "19    Please use the NLTK Downloader to obtain the...   \n",
       "20                               \u001b[31m>>> import nltk   \n",
       "21                         >>> nltk.download('words')   \n",
       "22                                               \u001b[0m   \n",
       "23                           For more information see   \n",
       "24           Attempted to load \u001b[93mcorpora/words\u001b[0m   \n",
       "25                                        Searched in   \n",
       "26                                               - 'C   \n",
       "27                                               - 'C   \n",
       "28                                               - 'C   \n",
       "29                                               - 'C   \n",
       "30                                               - 'C   \n",
       "31                                               - 'C   \n",
       "32                                               - 'D   \n",
       "33                                               - 'E   \n",
       "34  **********************************************...   \n",
       "\n",
       "                                            Type  \\\n",
       "0                                             45   \n",
       "1                                             53   \n",
       "2                                             09   \n",
       "3                                             24   \n",
       "4                                             39   \n",
       "5                                             54   \n",
       "6                                             09   \n",
       "7                                             24   \n",
       "8                                             39   \n",
       "9                                             54   \n",
       "10                                            54   \n",
       "11                                            09   \n",
       "12                                            09   \n",
       "13                                            12   \n",
       "14                                            12   \n",
       "15                                            13   \n",
       "16                                            13   \n",
       "17                                           NaN   \n",
       "18                                           NaN   \n",
       "19                                           NaN   \n",
       "20                                           NaN   \n",
       "21                                           NaN   \n",
       "22                                           NaN   \n",
       "23                                         https   \n",
       "24                                           NaN   \n",
       "25                                           NaN   \n",
       "26                     \\\\Users\\\\OnceU/nltk_data'   \n",
       "27          \\\\ProgramData\\\\Anaconda3\\\\nltk_data'   \n",
       "28   \\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'   \n",
       "29     \\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'   \n",
       "30  \\\\Users\\\\OnceU\\\\AppData\\\\Roaming\\\\nltk_data'   \n",
       "31                                  \\\\nltk_data'   \n",
       "32                                  \\\\nltk_data'   \n",
       "33                                  \\\\nltk_data'   \n",
       "34                                           NaN   \n",
       "\n",
       "                                              Message  Sleep  \n",
       "0     01 - WARNING - Rate limit reached. Sleeping for  484.0  \n",
       "1     55 - WARNING - Rate limit reached. Sleeping for  851.0  \n",
       "2     06 - WARNING - Rate limit reached. Sleeping for  841.0  \n",
       "3     09 - WARNING - Rate limit reached. Sleeping for  839.0  \n",
       "4     10 - WARNING - Rate limit reached. Sleeping for  839.0  \n",
       "5     11 - WARNING - Rate limit reached. Sleeping for  839.0  \n",
       "6     12 - WARNING - Rate limit reached. Sleeping for  839.0  \n",
       "7     13 - WARNING - Rate limit reached. Sleeping for  839.0  \n",
       "8     15 - WARNING - Rate limit reached. Sleeping for  838.0  \n",
       "9   09 - ERROR - Error occurred during tweet clean...    NaN  \n",
       "10              09 - ERROR - name 're' is not defined    NaN  \n",
       "11  39 - ERROR - Error occurred during tweet clean...    NaN  \n",
       "12              39 - ERROR - name 're' is not defined    NaN  \n",
       "13  07 - ERROR - Error occurred during tweet clean...    NaN  \n",
       "14           07 - ERROR - name 'words' is not defined    NaN  \n",
       "15  15 - ERROR - Error occurred during tweet clean...    NaN  \n",
       "16                                      15 - ERROR -     NaN  \n",
       "17                                                NaN    NaN  \n",
       "18                                                NaN    NaN  \n",
       "19                                                NaN    NaN  \n",
       "20                                                NaN    NaN  \n",
       "21                                                NaN    NaN  \n",
       "22                                                NaN    NaN  \n",
       "23                           //www.nltk.org/data.html    NaN  \n",
       "24                                                NaN    NaN  \n",
       "25                                                NaN    NaN  \n",
       "26                                                NaN    NaN  \n",
       "27                                                NaN    NaN  \n",
       "28                                                NaN    NaN  \n",
       "29                                                NaN    NaN  \n",
       "30                                                NaN    NaN  \n",
       "31                                                NaN    NaN  \n",
       "32                                                NaN    NaN  \n",
       "33                                                NaN    NaN  \n",
       "34                                                NaN    NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the log file into a DataFrame\n",
    "log_df = pd.read_csv(log_file, sep=\":\", names=[\"Timestamp\",\"Type\",\"Message\",\"Sleep\"])\n",
    "\n",
    "# Define the CSV file name for saving the log DataFrame\n",
    "csv_file = f\"TwitterAPI_{today}.csv\"\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "if os.path.isfile(csv_file):\n",
    "    # Append the log DataFrame to the existing CSV file\n",
    "    log_df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "else:\n",
    "    # Save the log DataFrame to a new CSV file\n",
    "    log_df.to_csv(csv_file, index=False)\n",
    "\n",
    "# Display the log DataFrame\n",
    "log_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482b2ac4",
   "metadata": {},
   "source": [
    "# Cleaning tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c079f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the tweets data from the CSV file\n",
    "    df_tweets = pd.read_csv('tweets.csv')\n",
    "\n",
    "    # Compile the regular expressions\n",
    "    remove_chars_regex = re.compile(r'[^\\w\\s]')\n",
    "    remove_links_regex = re.compile(r'http\\S+|www\\S+')\n",
    "    remove_usernames_regex = re.compile(r'@[^\\s]+')\n",
    "\n",
    "    # Function to clean a single tweet\n",
    "    def clean_tweet(tweet):\n",
    "        # Remove unnecessary characters and links\n",
    "        cleaned_tweet = remove_chars_regex.sub('', tweet)\n",
    "        cleaned_tweet = remove_links_regex.sub('', cleaned_tweet)\n",
    "\n",
    "        # Remove Twitter usernames\n",
    "        cleaned_tweet = remove_usernames_regex.sub('', cleaned_tweet)\n",
    "\n",
    "        # Remove non-English words\n",
    "        cleaned_words = []\n",
    "        english_words = set(words.words())\n",
    "        for word in cleaned_tweet.split():\n",
    "            if word.lower() in english_words:\n",
    "                cleaned_words.append(word)\n",
    "        cleaned_tweet = ' '.join(cleaned_words)\n",
    "\n",
    "        return cleaned_tweet\n",
    "\n",
    "    # Clean the tweets column using vectorization\n",
    "    df_tweets['Tweet'] = df_tweets['Tweet'].str.replace(remove_chars_regex, '')\n",
    "    df_tweets['Tweet'] = df_tweets['Tweet'].str.replace(remove_links_regex, '')\n",
    "    df_tweets['Tweet'] = df_tweets['Tweet'].str.replace(remove_usernames_regex, '')\n",
    "    df_tweets['Tweet'] = df_tweets['Tweet'].apply(clean_tweet)\n",
    "\n",
    "    # Remove rows with empty tweet values\n",
    "    df_tweets = df_tweets.dropna(subset=['Tweet'])\n",
    "\n",
    "    # Save the cleaned tweets back to the CSV file\n",
    "    df_tweets.to_csv('tweets.csv', index=False)\n",
    "\n",
    "    # Log the execution of the tweet cleaning process\n",
    "    logging.info(\"Tweet cleaning process completed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Log any exceptions or errors that occur\n",
    "    logging.error(\"Error occurred during tweet cleaning process.\")\n",
    "    logging.error(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fabffb7",
   "metadata": {},
   "source": [
    "# Tweet EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tweets data from the CSV file\n",
    "df_tweets = pd.read_csv('tweets.csv')\n",
    "# Perform basic exploratory data analysis (EDA)\n",
    "print(\"Number of tweets:\", len(df_tweets))\n",
    "print(\"Columns:\", df_tweets.columns)\n",
    "print(\"Sample tweets:\")\n",
    "print(df_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad86bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Perform sentiment analysis using Vader SentimentIntensityAnalyzer\n",
    "    nltk.download('vader_lexicon')\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    df_tweets['Sentiment'] = df_tweets['Tweet'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "    # Visualize sentiment distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(df_tweets['Sentiment'], bins=30, kde=True)\n",
    "    plt.title('Sentiment Distribution')\n",
    "    plt.xlabel('Sentiment Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # Log the successful execution of sentiment analysis and visualization\n",
    "    logging.info(\"Sentiment analysis and visualization completed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Log any exceptions or errors that occur\n",
    "    logging.error(\"Error occurred during sentiment analysis and visualization.\")\n",
    "    logging.error(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3472a30c",
   "metadata": {},
   "source": [
    "# Most frequent words wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b8e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word cloud of most frequent words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordcloud = WordCloud(stopwords=stop_words, background_color='white').generate(' '.join(df_tweets['Tweet']))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Most Frequent Words in Tweets')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6cc0d8",
   "metadata": {},
   "source": [
    "# Time series forecast of the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b28ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tweet sentiment data from the CSV file\n",
    "df = pd.read_csv('tweets_sentiment.csv', parse_dates=['Date'])\n",
    "\n",
    "# Set the 'Date' column as the index\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "try:\n",
    "    # Fit an ARIMA model to the sentiment data\n",
    "    model = sm.tsa.ARIMA(df['sentiment'], order=(1, 0, 1), trend='c').fit()\n",
    "\n",
    "    # Generate predictions for the next 1 week, 1 month, and 3 months\n",
    "    forecast_1w = model.predict(start=len(df), end=len(df) + 6, dynamic=False)\n",
    "    forecast_1m = model.predict(start=len(df), end=len(df) + 30, dynamic=False)\n",
    "    forecast_3m = model.predict(start=len(df), end=len(df) + 90, dynamic=False)\n",
    "\n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add actual sentiment data\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['sentiment'], name='Actual'))\n",
    "\n",
    "    # Add forecasted sentiment data\n",
    "    forecast_dates_1w = pd.date_range(start=df.index[-1], periods=7)[1:]\n",
    "    forecast_dates_1m = pd.date_range(start=df.index[-1], periods=31)[1:]\n",
    "    forecast_dates_3m = pd.date_range(start=df.index[-1], periods=91)[1:]\n",
    "    fig.add_trace(go.Scatter(x=forecast_dates_1w, y=forecast_1w, name='1 Week Forecast'))\n",
    "    fig.add_trace(go.Scatter(x=forecast_dates_1m, y=forecast_1m, name='1 Month Forecast'))\n",
    "    fig.add_trace(go.Scatter(x=forecast_dates_3m, y=forecast_3m, name='3 Months Forecast'))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Time Series Forecast of Sentiment',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Sentiment',\n",
    "        legend_title='Forecast',\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "\n",
    "    # Show the interactive Plotly graph\n",
    "    fig.show()\n",
    "\n",
    "except ValueError as e:\n",
    "    print(\"Error: Failed to make time series forecast.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e775c150",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e34aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the tweets from the CSV file\n",
    "    df = pd.read_csv('tweets.csv')\n",
    "\n",
    "    # Perform sentiment analysis using TextBlob\n",
    "    df['sentiment'] = df['Tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "    # Classify sentiment as positive, negative, or neutral\n",
    "    df['sentiment_label'] = df['sentiment'].apply(lambda x: 'Positive' if x > 0 else 'Negative' if x < 0 else 'Neutral')\n",
    "\n",
    "    # Save the updated DataFrame to CSV\n",
    "    df.to_csv('tweets_sentiment.csv', index=False)\n",
    "\n",
    "except pd.errors.EmptyDataError:\n",
    "    # Handle the case when the CSV file is empty\n",
    "    logging.error(\"Error: The CSV file is empty.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # Handle the case when the CSV file is not found\n",
    "    logging.error(\"Error: The CSV file 'tweets.csv' does not exist.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle any other exceptions that may occur\n",
    "    logging.error(\"An error occurred during sentiment analysis and classification.\")\n",
    "    logging.error(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f32f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert forecast data to strings\n",
    "forecast_1w_str = forecast_1w.to_string(header=False)\n",
    "forecast_1m_str = forecast_1m.to_string(header=False)\n",
    "forecast_3m_str = forecast_3m.to_string(header=False)\n",
    "# Print the forecast data\n",
    "print(\"1 Week Forecast:\")\n",
    "print(forecast_1w_str)\n",
    "print(\"1 Month Forecast:\")\n",
    "print(forecast_1m_str)\n",
    "print(\"3 Months Forecast:\")\n",
    "print(forecast_3m_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08cfbae",
   "metadata": {},
   "source": [
    "# Sentiment distribution by sentiment category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d34df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tweets data from the CSV file\n",
    "df_tweets = pd.read_csv('tweets_sentiment.csv')\n",
    "\n",
    "# Perform basic exploratory data analysis (EDA)\n",
    "print(\"Number of tweets:\", len(df_tweets))\n",
    "print(\"Columns:\", df_tweets.columns)\n",
    "print(\"Sample tweets:\")\n",
    "print(df_tweets.head())\n",
    "\n",
    "# Visualize sentiment distribution by sentiment category\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Sentiment', hue='Category', data=df_tweets)\n",
    "plt.title('Sentiment Distribution by Category')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bd8bfe",
   "metadata": {},
   "source": [
    "# Sentiment distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffc738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Sentiment', data=df_tweets)\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b924d413",
   "metadata": {},
   "source": [
    "# Sentiment distribution over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a1fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' column to datetime\n",
    "df_tweets['Date'] = pd.to_datetime(df_tweets['Date'])\n",
    "\n",
    "# Group by date and sentiment to calculate counts\n",
    "df_sentiment_counts = df_tweets.groupby(['Date', 'Sentiment']).size().reset_index(name='Count')\n",
    "\n",
    "# Pivot the data to have sentiment types as columns\n",
    "df_sentiment_pivot = df_sentiment_counts.pivot(index='Date', columns='Sentiment', values='Count')\n",
    "\n",
    "# Plot the sentiment distribution over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=df_sentiment_pivot, dashes=False)\n",
    "plt.title('Sentiment Distribution Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Sentiment')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21352f0a",
   "metadata": {},
   "source": [
    "# Most frequent words by sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372696a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the tweets into words\n",
    "tokenized_words = [word.lower() for tweet in df_tweets['Tweet'] for word in word_tokenize(tweet)]\n",
    "\n",
    "# Calculate the frequency distribution of words\n",
    "freq_dist = FreqDist(tokenized_words)\n",
    "most_common = freq_dist.most_common(20)\n",
    "\n",
    "# Plot the most frequent words by sentiment\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Count', y='Word', hue='Sentiment', data=pd.DataFrame(most_common, columns=['Word', 'Count']))\n",
    "plt.title('Most Frequent Words by Sentiment')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Word')\n",
    "plt.legend(title='Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5382c9",
   "metadata": {},
   "source": [
    "# PySpark tweet upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3823800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the values below with yours\n",
    "ACCESS_TOKEN = access_token\n",
    "ACCESS_SECRET = access_token_secret\n",
    "CONSUMER_KEY = consumer_key\n",
    "CONSUMER_SECRET = consumer_secret\n",
    "my_auth = requests_oauthlib.OAuth1(CONSUMER_KEY, CONSUMER_SECRET,ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "def get_tweets():\n",
    "    url = 'https://stream.twitter.com/1.1/statuses/filter.json'\n",
    "    query_data = [('language', 'en'), ('locations', '-130,-20,100,50'),('track','#')]\n",
    "    query_url = url + '?' + '&'.join([str(t[0]) + '=' + str(t[1]) for t in query_data])\n",
    "    response = requests.get(query_url, auth=my_auth, stream=True)\n",
    "    print(query_url, response)\n",
    "    return response\n",
    "\n",
    "def send_tweets_to_spark(http_resp, tcp_connection):\n",
    "    for line in http_resp.iter_lines():\n",
    "        try:\n",
    "            full_tweet = json.loads(line)\n",
    "            tweet_text = full_tweet['text']\n",
    "            print(\"Tweet Text: \" + tweet_text)\n",
    "            print (\"------------------------------------------\")\n",
    "            tcp_connection.send(tweet_text + '\\n')\n",
    "        except:\n",
    "            e = sys.exc_info()[0]\n",
    "            print(\"Error: %s\" % e)\n",
    "            \n",
    "TCP_IP = \"localhost\"\n",
    "TCP_PORT = 9009\n",
    "conn = None\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "s.bind((TCP_IP, TCP_PORT))\n",
    "s.listen(1)\n",
    "print(\"Waiting for TCP connection...\")\n",
    "conn, addr = s.accept()\n",
    "print(\"Connected... Starting getting tweets.\")\n",
    "resp = get_tweets()\n",
    "send_tweets_to_spark(resp, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca6f6b8",
   "metadata": {},
   "source": [
    "# Configure Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e6ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark configuration\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"TwitterStreamApp\")\n",
    "# create spark context with the above configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "# create the Streaming Context from the above spark context with interval size 2 seconds\n",
    "ssc = StreamingContext(sc, 2)\n",
    "# setting a checkpoint to allow RDD recovery\n",
    "ssc.checkpoint(\"checkpoint_TwitterApp\")\n",
    "# read data from port 9009\n",
    "dataStream = ssc.socketTextStream(\"localhost\",9009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641844ea",
   "metadata": {},
   "source": [
    "# Filter Hastags and stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e757d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each tweet into words\n",
    "words = dataStream.flatMap(lambda line: line.split(\" \"))\n",
    "# filter the words to get only hashtags, then map each hashtag to be a pair of (hashtag,1)\n",
    "hashtags = words.filter(lambda w: '#' in w).map(lambda x: (x, 1))\n",
    "# adding the count of each hashtag to its last count\n",
    "tags_totals = hashtags.updateStateByKey(aggregate_tags_count)\n",
    "# do processing for each RDD generated in each interval\n",
    "tags_totals.foreachRDD(process_rdd)\n",
    "# start the streaming computation\n",
    "ssc.start()\n",
    "# wait for the streaming to finish\n",
    "ssc.awaitTermination()\n",
    "\n",
    "def aggregate_tags_count(new_values, total_sum):\n",
    "    return sum(new_values) + (total_sum or 0)\n",
    "\n",
    "def get_sql_context_instance(spark_context):\n",
    "    if ('sqlContextSingletonInstance' not in globals()):\n",
    "        globals()['sqlContextSingletonInstance'] = SQLContext(spark_context)\n",
    "    return globals()['sqlContextSingletonInstance']\n",
    "def process_rdd(time, rdd):\n",
    "    print(\"----------- %s -----------\" % str(time))\n",
    "    try:\n",
    "        # Get spark sql singleton context from the current context\n",
    "        sql_context = get_sql_context_instance(rdd.context)\n",
    "        # convert the RDD to Row RDD\n",
    "        row_rdd = rdd.map(lambda w: Row(hashtag=w[0], hashtag_count=w[1]))\n",
    "        # create a DF from the Row RDD\n",
    "        hashtags_df = sql_context.createDataFrame(row_rdd)\n",
    "        # Register the dataframe as table\n",
    "        hashtags_df.registerTempTable(\"hashtags\")\n",
    "        # get the top 10 hashtags from the table using SQL and print them\n",
    "        hashtag_counts_df = sql_context.sql(\"select hashtag, hashtag_count from hashtags order by hashtag_count desc limit 10\")\n",
    "        hashtag_counts_df.show()\n",
    "        # call this method to prepare top 10 hashtags DF and send them\n",
    "        send_df_to_dashboard(hashtag_counts_df)\n",
    "    except:\n",
    "        e = sys.exc_info()[0]\n",
    "        print(\"Error: %s\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de7d73b",
   "metadata": {},
   "source": [
    "# Stream to dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b285568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_df_to_dashboard(df):\n",
    "    # extract the hashtags from dataframe and convert them into array\n",
    "    top_tags = [str(t.hashtag) for t in df.select(\"hashtag\").collect()]\n",
    "    # extract the counts from dataframe and convert them into array\n",
    "    tags_count = [p.hashtag_count for p in df.select(\"hashtag_count\").collect()]\n",
    "    # initialize and send the data through REST API\n",
    "    url = 'http://localhost:5001/updateData'\n",
    "    request_data = {'label': str(top_tags), 'data': str(tags_count)}\n",
    "    response = requests.post(url, data=request_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbbdee0",
   "metadata": {},
   "source": [
    "# Building the Flask app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bd6fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "labels = []\n",
    "values = []\n",
    "@app.route(\"/\")\n",
    "def get_chart_page():\n",
    "    global labels,values\n",
    "    labels = []\n",
    "    values = []\n",
    "    return render_template('chart.html', values=values, labels=labels)\n",
    "@app.route('/refreshData')\n",
    "def refresh_graph_data():\n",
    "    global labels, values\n",
    "    print(\"labels now: \" + str(labels))\n",
    "    print(\"data now: \" + str(values))\n",
    "    return jsonify(sLabel=labels, sData=values)\n",
    "@app.route('/updateData', methods=['POST'])\n",
    "def update_data():\n",
    "    global labels, values\n",
    "    if not request.form or 'data' not in request.form:\n",
    "        return \"error\",400\n",
    "    labels = ast.literal_eval(request.form['label'])\n",
    "    values = ast.literal_eval(request.form['data'])\n",
    "    print(\"labels received: \" + str(labels))\n",
    "    print(\"data received: \" + str(values))\n",
    "    return \"success\",201\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='localhost', port=5001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
