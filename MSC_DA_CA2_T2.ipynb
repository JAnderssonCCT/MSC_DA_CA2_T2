{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217e0670",
   "metadata": {},
   "source": [
    "# Download libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f0605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pytz\n",
    "!pip install tweepy\n",
    "!pip install statsmodels\n",
    "!pip install plotly\n",
    "!pip install wordcloud\n",
    "!pip install textblob\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8bf5c8",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "from pytz import timezone\n",
    "import tweepy\n",
    "import json\n",
    "import statsmodels.api as sm\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "from pandas.errors import EmptyDataError\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85e1ba",
   "metadata": {},
   "source": [
    "# Loading the credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65039743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Twitter API credentials from the config file\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    consumer_key = config['consumer_key']\n",
    "    consumer_secret = config['consumer_secret']\n",
    "    access_token = config['access_token']\n",
    "    access_token_secret = config['access_token_secret']\n",
    "    \n",
    "# Verify the Twitter API credentials\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "try:\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "    user = api.verify_credentials()\n",
    "    print(\"Twitter API connection successful.\")\n",
    "except tweepy.error.TweepError as e:\n",
    "    print(\"Error: Failed to verify Twitter API credentials.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d1610",
   "metadata": {},
   "source": [
    "# Disabling warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374410ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c567c",
   "metadata": {},
   "source": [
    "# Downloading and save twitter data\n",
    "The free tier of the twitter API holds the limitation of:</br>\n",
    "<b>**7 Day tweet history limit </br>\n",
    "**1500 tweet request limit </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a17cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get today's date\n",
    "today = datetime.now().date()\n",
    "\n",
    "# Create a log file name with today's date\n",
    "log_file = f\"TwitterAPI_{today}.log\"\n",
    "\n",
    "# Check if the log file exists\n",
    "if os.path.isfile(log_file):\n",
    "    # Append logs to the existing file\n",
    "    sys.stdout = open(log_file, \"a\")\n",
    "else:\n",
    "    # Create a new log file\n",
    "    sys.stdout = open(log_file, \"w\")\n",
    "\n",
    "# Define the topic and initial date range\n",
    "topic = \"(ios OR apple OR AAPL OR iphone OR ipad)\"\n",
    "start_date = today - timedelta(days=91)\n",
    "\n",
    "# Create a loop to run for 91 days\n",
    "for _ in range(91):\n",
    "    # Calculate the end date for the current iteration\n",
    "    end_date = start_date + timedelta(days=1)\n",
    "    \n",
    "    # Format the dates as strings\n",
    "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Define the search query with the current date range\n",
    "    query = f\"{topic} until:{end_date_str} since:{start_date_str}\"\n",
    "    \n",
    "    # Fetch tweets on the specified topic\n",
    "    try:\n",
    "        tweets = []\n",
    "        for tweet in tweepy.Cursor(api.search_tweets, q=query, lang='en', tweet_mode='extended').items(3000):\n",
    "            tweets.append({\n",
    "                'Date': tweet.created_at.date(),\n",
    "                'Tweet': tweet.full_text\n",
    "            })\n",
    "        \n",
    "        if len(tweets) > 0:\n",
    "            print(\"Tweets downloaded successfully for the date range:\", start_date_str, \"to\", end_date_str)\n",
    "            \n",
    "            # Convert the tweets list into a DataFrame\n",
    "            df_new = pd.DataFrame(tweets)\n",
    "            \n",
    "            # Check if the CSV file already exists\n",
    "            if os.path.isfile('tweets.csv'):\n",
    "                # Read the existing data from the CSV file\n",
    "                try:\n",
    "                    df_existing = pd.read_csv('tweets.csv')\n",
    "                    \n",
    "                    # Check if the existing DataFrame has any columns\n",
    "                    if df_existing.columns.empty:\n",
    "                        # Handle the case when the CSV file is empty\n",
    "                        df_existing = pd.DataFrame()\n",
    "                        \n",
    "                except pd.errors.EmptyDataError:\n",
    "                    # Handle the case when the CSV file is empty\n",
    "                    df_existing = pd.DataFrame()\n",
    "                \n",
    "                # Check if the existing DataFrame is empty\n",
    "                if df_existing.empty:\n",
    "                    # Save the new DataFrame to a new CSV file\n",
    "                    df_new.to_csv('tweets.csv', index=False)\n",
    "                    print(\"New CSV file created with the downloaded tweets.\")\n",
    "                else:\n",
    "                    # Concatenate the existing and new data\n",
    "                    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "                    \n",
    "                    # Save the combined DataFrame to the CSV file\n",
    "                    df_combined.to_csv('tweets.csv', index=False)\n",
    "                    print(\"Tweets appended to the existing CSV file.\")\n",
    "            else:\n",
    "                # Save the new DataFrame to a new CSV file\n",
    "                df_new.to_csv('tweets.csv', index=False)\n",
    "                print(\"New CSV file created with the downloaded tweets.\")\n",
    "        else:\n",
    "            print(\"No tweets found for the date range:\", start_date_str, \"to\", end_date_str)\n",
    "            \n",
    "    except tweepy.TweepyException as e:\n",
    "        if e.api_code == 88:\n",
    "            # Rate limit reached, wait for the specified duration\n",
    "            wait_time = int(e.response.headers['Retry-After'])\n",
    "            print(\"Rate limit reached. Sleeping for:\", wait_time, \"seconds.\")\n",
    "            time.sleep(wait_time)\n",
    "        print(\"Error: Failed to download tweets.\")\n",
    "        print(e)\n",
    "    \n",
    "    # Update the start date for the next iteration\n",
    "    start_date = end_date\n",
    "\n",
    "# Close the log file\n",
    "sys.stdout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea4e14",
   "metadata": {},
   "source": [
    "# Log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ef479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the log file into a DataFrame\n",
    "log_df = pd.read_csv(\"log_file\", sep=\":\", names=[\"Timestamp\", \"Log Message\"])\n",
    "# Display the log DataFrame\n",
    "log_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697c1be4",
   "metadata": {},
   "source": [
    "# Cleaning tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2773418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tweets data from the CSV file\n",
    "df_tweets = pd.read_csv('tweets.csv')\n",
    "\n",
    "# Function to clean a single tweet\n",
    "def clean_tweet(tweet):\n",
    "    # Remove unnecessary characters and links\n",
    "    cleaned_tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "    cleaned_tweet = re.sub(r'http\\S+|www\\S+', '', cleaned_tweet)\n",
    "    \n",
    "    # Remove Twitter usernames\n",
    "    cleaned_tweet = re.sub(r'@[^\\s]+', '', cleaned_tweet)\n",
    "    \n",
    "    # Remove non-English words\n",
    "    cleaned_words = []\n",
    "    english_words = set(words.words())\n",
    "    for word in cleaned_tweet.split():\n",
    "        if word.lower() in english_words:\n",
    "            cleaned_words.append(word)\n",
    "    cleaned_tweet = ' '.join(cleaned_words)\n",
    "    \n",
    "    return cleaned_tweet\n",
    "\n",
    "# Clean the tweets column\n",
    "df_tweets['Tweet'] = df_tweets['Tweet'].apply(clean_tweet)\n",
    "\n",
    "# Save the cleaned tweets back to the CSV file\n",
    "df_tweets.to_csv('tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35806786",
   "metadata": {},
   "source": [
    "# Tweet EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db670e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tweets data from the CSV file\n",
    "df_tweets = pd.read_csv('tweets.csv')\n",
    "# Perform basic exploratory data analysis (EDA)\n",
    "print(\"Number of tweets:\", len(df_tweets))\n",
    "print(\"Columns:\", df_tweets.columns)\n",
    "print(\"Sample tweets:\")\n",
    "print(df_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5714fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sentiment analysis using Vader SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "df_tweets['Sentiment'] = df_tweets['Tweet'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "# Visualize sentiment distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df_tweets['Sentiment'], bins=30, kde=True)\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf36c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word cloud of most frequent words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordcloud = WordCloud(stopwords=stop_words, background_color='white').generate(' '.join(df_tweets['Tweet']))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Most Frequent Words in Tweets')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b9608",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e34aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tweets from the CSV file\n",
    "df = pd.read_csv('tweets.csv')\n",
    "\n",
    "# Perform sentiment analysis using TextBlob\n",
    "df['sentiment'] = df['Tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Classify sentiment as positive, negative, or neutral\n",
    "df['sentiment_label'] = df['sentiment'].apply(lambda x: 'Positive' if x > 0 else 'Negative' if x < 0 else 'Neutral')\n",
    "\n",
    "# Save the updated DataFrame to CSV\n",
    "df.to_csv('tweets_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec20864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert forecast data to strings\n",
    "forecast_1w_str = forecast_1w.to_string(header=False)\n",
    "forecast_1m_str = forecast_1m.to_string(header=False)\n",
    "forecast_3m_str = forecast_3m.to_string(header=False)\n",
    "# Print the forecast data\n",
    "print(\"1 Week Forecast:\")\n",
    "print(forecast_1w_str)\n",
    "print(\"1 Month Forecast:\")\n",
    "print(forecast_1m_str)\n",
    "print(\"3 Months Forecast:\")\n",
    "print(forecast_3m_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f0da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tweets data from the CSV file\n",
    "df_tweets = pd.read_csv('tweets_sentiment.csv')\n",
    "\n",
    "# Perform basic exploratory data analysis (EDA)\n",
    "print(\"Number of tweets:\", len(df_tweets))\n",
    "print(\"Columns:\", df_tweets.columns)\n",
    "print(\"Sample tweets:\")\n",
    "print(df_tweets.head())\n",
    "\n",
    "# Visualize sentiment distribution by sentiment category\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Sentiment', hue='Category', data=df_tweets)\n",
    "plt.title('Sentiment Distribution by Category')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Sentiment', data=df_tweets)\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb93e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' column to datetime\n",
    "df_tweets['Date'] = pd.to_datetime(df_tweets['Date'])\n",
    "\n",
    "# Group by date and sentiment to calculate counts\n",
    "df_sentiment_counts = df_tweets.groupby(['Date', 'Sentiment']).size().reset_index(name='Count')\n",
    "\n",
    "# Pivot the data to have sentiment types as columns\n",
    "df_sentiment_pivot = df_sentiment_counts.pivot(index='Date', columns='Sentiment', values='Count')\n",
    "\n",
    "# Plot the sentiment distribution over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=df_sentiment_pivot, dashes=False)\n",
    "plt.title('Sentiment Distribution Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Sentiment')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce96bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the tweets into words\n",
    "tokenized_words = [word.lower() for tweet in df_tweets['Tweet'] for word in word_tokenize(tweet)]\n",
    "\n",
    "# Calculate the frequency distribution of words\n",
    "freq_dist = FreqDist(tokenized_words)\n",
    "most_common = freq_dist.most_common(20)\n",
    "\n",
    "# Plot the most frequent words by sentiment\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Count', y='Word', hue='Sentiment', data=pd.DataFrame(most_common, columns=['Word', 'Count']))\n",
    "plt.title('Most Frequent Words by Sentiment')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Word')\n",
    "plt.legend(title='Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ad8d6",
   "metadata": {},
   "source": [
    "# Time series forecast of the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3693114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tweet sentiment data from the CSV file\n",
    "df = pd.read_csv('tweets_sentiment.csv', parse_dates=['Date'])\n",
    "\n",
    "# Set the 'Date' column as the index\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "try:\n",
    "    # Fit an ARIMA model to the sentiment data\n",
    "    model = sm.tsa.ARIMA(df['sentiment'], order=(1, 0, 1), trend='c').fit()\n",
    "\n",
    "    # Generate predictions for the next 1 week, 1 month, and 3 months\n",
    "    forecast_1w = model.predict(start=len(df), end=len(df) + 6, dynamic=False)\n",
    "    forecast_1m = model.predict(start=len(df), end=len(df) + 30, dynamic=False)\n",
    "    forecast_3m = model.predict(start=len(df), end=len(df) + 90, dynamic=False)\n",
    "\n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add actual sentiment data\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['sentiment'], name='Actual'))\n",
    "\n",
    "    # Add forecasted sentiment data\n",
    "    forecast_dates_1w = pd.date_range(start=df.index[-1], periods=7)[1:]\n",
    "    forecast_dates_1m = pd.date_range(start=df.index[-1], periods=31)[1:]\n",
    "    forecast_dates_3m = pd.date_range(start=df.index[-1], periods=91)[1:]\n",
    "    fig.add_trace(go.Scatter(x=forecast_dates_1w, y=forecast_1w, name='1 Week Forecast'))\n",
    "    fig.add_trace(go.Scatter(x=forecast_dates_1m, y=forecast_1m, name='1 Month Forecast'))\n",
    "    fig.add_trace(go.Scatter(x=forecast_dates_3m, y=forecast_3m, name='3 Months Forecast'))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Time Series Forecast of Sentiment',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Sentiment',\n",
    "        legend_title='Forecast',\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "\n",
    "    # Show the interactive Plotly graph\n",
    "    fig.show()\n",
    "\n",
    "except ValueError as e:\n",
    "    print(\"Error: Failed to make time series forecast.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4267ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
