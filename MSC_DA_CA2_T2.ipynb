{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217e0670",
   "metadata": {},
   "source": [
    "# Download libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f0605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pytz\n",
    "!pip install tweepy\n",
    "!pip install statsmodels\n",
    "!pip install plotly\n",
    "!pip install wordcloud\n",
    "!pip install textblob\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8bf5c8",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "from pytz import timezone\n",
    "import tweepy\n",
    "import json\n",
    "import statsmodels.api as sm\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "from pandas.errors import EmptyDataError\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85e1ba",
   "metadata": {},
   "source": [
    "# Loading the credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65039743",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the Twitter API credentials from the config file\n",
    "    with open('config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "        consumer_key = config['consumer_key']\n",
    "        consumer_secret = config['consumer_secret']\n",
    "        access_token = config['access_token']\n",
    "        access_token_secret = config['access_token_secret']\n",
    "\n",
    "    # Verify the Twitter API credentials\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "    try:\n",
    "        api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "        user = api.verify_credentials()\n",
    "        logging.info(\"Twitter API connection successful.\")\n",
    "    except tweepy.error.TweepError as e:\n",
    "        logging.error(\"Error: Failed to verify Twitter API credentials.\")\n",
    "        logging.error(str(e))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Error: The config file 'config.json' does not exist.\")\n",
    "\n",
    "except json.JSONDecodeError as e:\n",
    "    logging.error(\"Error: Failed to load Twitter API credentials from 'config.json'.\")\n",
    "    logging.error(str(e))\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(\"An error occurred during a Twitter API connection.\")\n",
    "    logging.error(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d1610",
   "metadata": {},
   "source": [
    "# Disabling warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374410ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a0a30d",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2745e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get today's date\n",
    "today = datetime.now().date()\n",
    "\n",
    "# Create a log file with today's date in the name\n",
    "log_file = f\"TwitterAPI_{today}.log\"\n",
    "\n",
    "# Configure the root logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Create a FileHandler and set its formatter\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s','%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# Attach the FileHandler to the root logger\n",
    "logging.getLogger().addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c567c",
   "metadata": {},
   "source": [
    "# Downloading and save twitter data\n",
    "The free tier of the twitter API holds the limitation of:</br>\n",
    "<b>**10 Day tweet history limit </br>\n",
    "**1500 tweet request limit per 900sec circ. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a17cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the topic and initial date range\n",
    "topic = \"(ios OR apple OR AAPL OR iphone OR ipad)\"\n",
    "start_date = today - timedelta(days=91)\n",
    "\n",
    "# Create a loop to run for 91 days\n",
    "for _ in range(91):\n",
    "    # Calculate the end date for the current iteration\n",
    "    end_date = start_date + timedelta(days=1)\n",
    "    \n",
    "    # Format the dates as strings\n",
    "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Define the search query with the current date range\n",
    "    query = f\"{topic} until:{end_date_str} since:{start_date_str}\"\n",
    "    \n",
    "    # Fetch tweets on the specified topic\n",
    "    try:\n",
    "        tweets = []\n",
    "        for tweet in tweepy.Cursor(api.search_tweets, q=query, lang='en', tweet_mode='extended').items(3000):\n",
    "            tweets.append({\n",
    "                'Date': tweet.created_at.date(),\n",
    "                'Tweet': tweet.full_text\n",
    "            })\n",
    "        \n",
    "        if len(tweets) > 0:\n",
    "            msg = \"Tweets downloaded successfully for the date range: {} to {}\"\n",
    "            logging.info(msg.format(start_date_str, end_date_str))\n",
    "            \n",
    "            # Convert the tweets list into a DataFrame\n",
    "            df_new = pd.DataFrame(tweets)\n",
    "            \n",
    "            # Check if the CSV file already exists\n",
    "            if os.path.isfile('tweets.csv'):\n",
    "                # Read the existing data from the CSV file\n",
    "                try:\n",
    "                    df_existing = pd.read_csv('tweets.csv')\n",
    "                    \n",
    "                    # Check if the existing DataFrame has any columns\n",
    "                    if df_existing.columns.empty:\n",
    "                        # Handle the case when the CSV file is empty\n",
    "                        df_existing = pd.DataFrame()\n",
    "                        \n",
    "                except EmptyDataError:\n",
    "                    # Handle the case when the CSV file is empty\n",
    "                    df_existing = pd.DataFrame()\n",
    "                \n",
    "                # Check if the existing DataFrame is empty\n",
    "                if df_existing.empty:\n",
    "                    # Save the new DataFrame to a new CSV file\n",
    "                    df_new.to_csv('tweets.csv', index=False)\n",
    "                    logging.info(\"New CSV file created with the downloaded tweets.\")\n",
    "                else:\n",
    "                    # Concatenate the existing and new data\n",
    "                    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "                    \n",
    "                    # Save the combined DataFrame to the CSV file\n",
    "                    df_combined.to_csv('tweets.csv', index=False)\n",
    "                    logging.info(\"Tweets appended to the existing CSV file.\")\n",
    "            else:\n",
    "                # Save the new DataFrame to a new CSV file\n",
    "                df_new.to_csv('tweets.csv', index=False)\n",
    "                logging.info(\"New CSV file created with the downloaded tweets.\")\n",
    "        else:\n",
    "            msg = \"No tweets found for the date range: {} to {}\"\n",
    "            logging.info(msg.format(start_date_str, end_date_str))\n",
    "            \n",
    "    except tweepy.TweepyException as e:\n",
    "        if e.api_code == 88:\n",
    "            # Rate limit reached, wait for the specified duration\n",
    "            wait_time = int(e.response.headers['Retry-After'])\n",
    "            msg = \"Rate limit reached. Sleeping for: {} seconds.\"\n",
    "            logging.info(msg.format(wait_time))\n",
    "            time.sleep(wait_time)\n",
    "        logging.error(\"Error: Failed to download tweets.\")\n",
    "        logging.error(e)\n",
    "    \n",
    "    # Update the start date for the next iteration\n",
    "    start_date = end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f23fea",
   "metadata": {},
   "source": [
    "# Log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10531093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the log file into a DataFrame\n",
    "log_df = pd.read_csv(log_file, sep=\":\", names=[\"Type\",\"User\",\"Log MSG\", \"Timestamp\"])\n",
    "\n",
    "# Define the CSV file name for saving the log DataFrame\n",
    "csv_file = f\"TwitterAPI_{today}.csv\"\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "if os.path.isfile(csv_file):\n",
    "    # Append the log DataFrame to the existing CSV file\n",
    "    log_df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "else:\n",
    "    # Save the log DataFrame to a new CSV file\n",
    "    log_df.to_csv(csv_file, index=False)\n",
    "\n",
    "# Display the log DataFrame\n",
    "log_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482b2ac4",
   "metadata": {},
   "source": [
    "# Cleaning tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c079f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the tweets data from the CSV file\n",
    "    df_tweets = pd.read_csv('tweets.csv')\n",
    "\n",
    "    # Function to clean a single tweet\n",
    "    def clean_tweet(tweet):\n",
    "        # Remove unnecessary characters and links\n",
    "        cleaned_tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "        cleaned_tweet = re.sub(r'http\\S+|www\\S+', '', cleaned_tweet)\n",
    "\n",
    "        # Remove Twitter usernames\n",
    "        cleaned_tweet = re.sub(r'@[^\\s]+', '', cleaned_tweet)\n",
    "\n",
    "        # Remove non-English words\n",
    "        cleaned_words = []\n",
    "        english_words = set(words.words())\n",
    "        for word in cleaned_tweet.split():\n",
    "            if word.lower() in english_words:\n",
    "                cleaned_words.append(word)\n",
    "        cleaned_tweet = ' '.join(cleaned_words)\n",
    "\n",
    "        return cleaned_tweet\n",
    "\n",
    "    # Clean the tweets column\n",
    "    df_tweets['Tweet'] = df_tweets['Tweet'].apply(clean_tweet)\n",
    "\n",
    "    # Remove rows with empty tweet values\n",
    "    df_tweets = df_tweets.dropna(subset=['Tweet'])\n",
    "\n",
    "    # Save the cleaned tweets back to the CSV file\n",
    "    df_tweets.to_csv('tweets.csv', index=False)\n",
    "\n",
    "    # Log the execution of the tweet cleaning process\n",
    "    logging.info(\"Tweet cleaning process completed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Log any exceptions or errors that occur\n",
    "    logging.error(\"Error occurred during tweet cleaning process.\")\n",
    "    logging.error(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fabffb7",
   "metadata": {},
   "source": [
    "# Tweet EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tweets data from the CSV file\n",
    "df_tweets = pd.read_csv('tweets.csv')\n",
    "# Perform basic exploratory data analysis (EDA)\n",
    "print(\"Number of tweets:\", len(df_tweets))\n",
    "print(\"Columns:\", df_tweets.columns)\n",
    "print(\"Sample tweets:\")\n",
    "print(df_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad86bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Perform sentiment analysis using Vader SentimentIntensityAnalyzer\n",
    "    nltk.download('vader_lexicon')\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    df_tweets['Sentiment'] = df_tweets['Tweet'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "    # Visualize sentiment distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(df_tweets['Sentiment'], bins=30, kde=True)\n",
    "    plt.title('Sentiment Distribution')\n",
    "    plt.xlabel('Sentiment Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # Log the successful execution of sentiment analysis and visualization\n",
    "    logging.info(\"Sentiment analysis and visualization completed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Log any exceptions or errors that occur\n",
    "    logging.error(\"Error occurred during sentiment analysis and visualization.\")\n",
    "    logging.error(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b8e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word cloud of most frequent words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordcloud = WordCloud(stopwords=stop_words, background_color='white').generate(' '.join(df_tweets['Tweet']))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Most Frequent Words in Tweets')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b9608",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e34aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the tweets from the CSV file\n",
    "    df = pd.read_csv('tweets.csv')\n",
    "\n",
    "    # Perform sentiment analysis using TextBlob\n",
    "    df['sentiment'] = df['Tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "    # Classify sentiment as positive, negative, or neutral\n",
    "    df['sentiment_label'] = df['sentiment'].apply(lambda x: 'Positive' if x > 0 else 'Negative' if x < 0 else 'Neutral')\n",
    "\n",
    "    # Save the updated DataFrame to CSV\n",
    "    df.to_csv('tweets_sentiment.csv', index=False)\n",
    "\n",
    "except pd.errors.EmptyDataError:\n",
    "    # Handle the case when the CSV file is empty\n",
    "    logging.error(\"Error: The CSV file is empty.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # Handle the case when the CSV file is not found\n",
    "    logging.error(\"Error: The CSV file 'tweets.csv' does not exist.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle any other exceptions that may occur\n",
    "    logging.error(\"An error occurred during sentiment analysis and classification.\")\n",
    "    logging.error(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f32f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert forecast data to strings\n",
    "forecast_1w_str = forecast_1w.to_string(header=False)\n",
    "forecast_1m_str = forecast_1m.to_string(header=False)\n",
    "forecast_3m_str = forecast_3m.to_string(header=False)\n",
    "# Print the forecast data\n",
    "print(\"1 Week Forecast:\")\n",
    "print(forecast_1w_str)\n",
    "print(\"1 Month Forecast:\")\n",
    "print(forecast_1m_str)\n",
    "print(\"3 Months Forecast:\")\n",
    "print(forecast_3m_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d34df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tweets data from the CSV file\n",
    "df_tweets = pd.read_csv('tweets_sentiment.csv')\n",
    "\n",
    "# Perform basic exploratory data analysis (EDA)\n",
    "print(\"Number of tweets:\", len(df_tweets))\n",
    "print(\"Columns:\", df_tweets.columns)\n",
    "print(\"Sample tweets:\")\n",
    "print(df_tweets.head())\n",
    "\n",
    "# Visualize sentiment distribution by sentiment category\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Sentiment', hue='Category', data=df_tweets)\n",
    "plt.title('Sentiment Distribution by Category')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffc738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Sentiment', data=df_tweets)\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a1fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' column to datetime\n",
    "df_tweets['Date'] = pd.to_datetime(df_tweets['Date'])\n",
    "\n",
    "# Group by date and sentiment to calculate counts\n",
    "df_sentiment_counts = df_tweets.groupby(['Date', 'Sentiment']).size().reset_index(name='Count')\n",
    "\n",
    "# Pivot the data to have sentiment types as columns\n",
    "df_sentiment_pivot = df_sentiment_counts.pivot(index='Date', columns='Sentiment', values='Count')\n",
    "\n",
    "# Plot the sentiment distribution over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=df_sentiment_pivot, dashes=False)\n",
    "plt.title('Sentiment Distribution Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Sentiment')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372696a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the tweets into words\n",
    "tokenized_words = [word.lower() for tweet in df_tweets['Tweet'] for word in word_tokenize(tweet)]\n",
    "\n",
    "# Calculate the frequency distribution of words\n",
    "freq_dist = FreqDist(tokenized_words)\n",
    "most_common = freq_dist.most_common(20)\n",
    "\n",
    "# Plot the most frequent words by sentiment\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Count', y='Word', hue='Sentiment', data=pd.DataFrame(most_common, columns=['Word', 'Count']))\n",
    "plt.title('Most Frequent Words by Sentiment')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Word')\n",
    "plt.legend(title='Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ad8d6",
   "metadata": {},
   "source": [
    "# Time series forecast of the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3693114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tweet sentiment data from the CSV file\n",
    "df = pd.read_csv('tweets_sentiment.csv', parse_dates=['Date'])\n",
    "\n",
    "# Set the 'Date' column as the index\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "try:\n",
    "    # Fit an ARIMA model to the sentiment data\n",
    "    model = sm.tsa.ARIMA(df['sentiment'], order=(1, 0, 1), trend='c').fit()\n",
    "\n",
    "    # Generate predictions for the next 1 week, 1 month, and 3 months\n",
    "    forecast_1w = model.predict(start=len(df), end=len(df) + 6, dynamic=False)\n",
    "    forecast_1m = model.predict(start=len(df), end=len(df) + 30, dynamic=False)\n",
    "    forecast_3m = model.predict(start=len(df), end=len(df) + 90, dynamic=False)\n",
    "\n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add actual sentiment data\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['sentiment'], name='Actual'))\n",
    "\n",
    "    # Add forecasted sentiment data\n",
    "    forecast_dates_1w = pd.date_range(start=df.index[-1], periods=7)[1:]\n",
    "    forecast_dates_1m = pd.date_range(start=df.index[-1], periods=31)[1:]\n",
    "    forecast_dates_3m = pd.date_range(start=df.index[-1], periods=91)[1:]\n",
    "    fig.add_trace(go.Scatter(x=forecast_dates_1w, y=forecast_1w, name='1 Week Forecast'))\n",
    "    fig.add_trace(go.Scatter(x=forecast_dates_1m, y=forecast_1m, name='1 Month Forecast'))\n",
    "    fig.add_trace(go.Scatter(x=forecast_dates_3m, y=forecast_3m, name='3 Months Forecast'))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Time Series Forecast of Sentiment',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Sentiment',\n",
    "        legend_title='Forecast',\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "\n",
    "    # Show the interactive Plotly graph\n",
    "    fig.show()\n",
    "\n",
    "except ValueError as e:\n",
    "    print(\"Error: Failed to make time series forecast.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f533dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
