{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217e0670",
   "metadata": {},
   "source": [
    "# Download libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "268f0605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytz in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (2022.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tweepy in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (4.12.1)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (2.27.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from requests<3,>=2.27.0->tweepy) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: statsmodels in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (0.13.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from statsmodels) (1.21.6)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from statsmodels) (0.5.3)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from statsmodels) (21.3)\n",
      "Requirement already satisfied: scipy>=1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from statsmodels) (1.7.3)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from statsmodels) (1.5.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=21.3->statsmodels) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.25->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from pandas>=0.25->statsmodels) (2022.7)\n",
      "Requirement already satisfied: six in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: plotly in c:\\programdata\\anaconda3\\lib\\site-packages (5.6.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from plotly) (8.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from plotly) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wordcloud in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (1.9.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from wordcloud) (1.21.6)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (3.5.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from wordcloud) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->wordcloud) (4.38.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textblob in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.0)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.3.15)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\onceu\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytz\n",
    "!pip install tweepy\n",
    "!pip install statsmodels\n",
    "!pip install plotly\n",
    "!pip install wordcloud\n",
    "!pip install textblob\n",
    "!pip install nltk\n",
    "!pip install requests_oauthlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8bf5c8",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30ec967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "from pytz import timezone\n",
    "import tweepy\n",
    "import json\n",
    "import statsmodels.api as sm\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "from pandas.errors import EmptyDataError\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "import socket\n",
    "import requests_oauthlib\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85e1ba",
   "metadata": {},
   "source": [
    "# Loading the credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65039743",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the Twitter API credentials from the config file\n",
    "    with open('config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "        consumer_key = config['consumer_key']\n",
    "        consumer_secret = config['consumer_secret']\n",
    "        access_token = config['access_token']\n",
    "        access_token_secret = config['access_token_secret']\n",
    "\n",
    "    # Verify the Twitter API credentials\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "    try:\n",
    "        api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "        user = api.verify_credentials()\n",
    "        logging.info(\"Twitter API connection successful.\")\n",
    "    except tweepy.error.TweepError as e:\n",
    "        logging.error(\"Error: Failed to verify Twitter API credentials.\")\n",
    "        logging.error(str(e))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Error: The config file 'config.json' does not exist.\")\n",
    "\n",
    "except json.JSONDecodeError as e:\n",
    "    logging.error(\"Error: Failed to load Twitter API credentials from 'config.json'.\")\n",
    "    logging.error(str(e))\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(\"An error occurred during a Twitter API connection.\")\n",
    "    logging.error(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d1610",
   "metadata": {},
   "source": [
    "# Disabling warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "374410ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b13ad",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a85e5fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get today's date\n",
    "today = datetime.now().date()\n",
    "\n",
    "# Create a log file with today's date in the name\n",
    "log_file = f\"TwitterAPI_{today}.log\"\n",
    "\n",
    "# Configure the root logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Create a FileHandler and set its formatter\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s','%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# Attach the FileHandler to the root logger\n",
    "logging.getLogger().addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c567c",
   "metadata": {},
   "source": [
    "# Downloading and save twitter data\n",
    "The free tier of the twitter API holds the limitation of:</br>\n",
    "<b>**10 Day tweet history limit </br>\n",
    "**1500 tweet request limit per 900sec circ. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a17cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tweepy.api:Rate limit reached. Sleeping for: 484\n",
      "WARNING:tweepy.api:Rate limit reached. Sleeping for: 851\n",
      "WARNING:tweepy.api:Rate limit reached. Sleeping for: 841\n",
      "WARNING:tweepy.api:Rate limit reached. Sleeping for: 839\n",
      "WARNING:tweepy.api:Rate limit reached. Sleeping for: 839\n",
      "WARNING:tweepy.api:Rate limit reached. Sleeping for: 839\n"
     ]
    }
   ],
   "source": [
    "# Define the topic and initial date range\n",
    "topic = \"(ios OR apple OR AAPL OR iphone OR ipad)\"\n",
    "start_date = today - timedelta(days=91)\n",
    "\n",
    "# Create a loop to run for 91 days\n",
    "for _ in range(91):\n",
    "    # Calculate the end date for the current iteration\n",
    "    end_date = start_date + timedelta(days=1)\n",
    "    \n",
    "    # Format the dates as strings\n",
    "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Define the search query with the current date range\n",
    "    query = f\"{topic} until:{end_date_str} since:{start_date_str}\"\n",
    "    \n",
    "    # Fetch tweets on the specified topic\n",
    "    try:\n",
    "        tweets = []\n",
    "        for tweet in tweepy.Cursor(api.search_tweets, q=query, lang='en', tweet_mode='extended').items(3000):\n",
    "            tweets.append({\n",
    "                'Date': tweet.created_at.date(),\n",
    "                'Tweet': tweet.full_text\n",
    "            })\n",
    "        \n",
    "        if len(tweets) > 0:\n",
    "            msg = \"Tweets downloaded successfully for the date range: {} to {}\"\n",
    "            logging.info(msg.format(start_date_str, end_date_str))\n",
    "            \n",
    "            # Convert the tweets list into a DataFrame\n",
    "            df_new = pd.DataFrame(tweets)\n",
    "            \n",
    "            # Check if the CSV file already exists\n",
    "            if os.path.isfile('tweets.csv'):\n",
    "                # Read the existing data from the CSV file\n",
    "                try:\n",
    "                    df_existing = pd.read_csv('tweets.csv')\n",
    "                    \n",
    "                    # Check if the existing DataFrame has any columns\n",
    "                    if df_existing.columns.empty:\n",
    "                        # Handle the case when the CSV file is empty\n",
    "                        df_existing = pd.DataFrame()\n",
    "                        \n",
    "                except EmptyDataError:\n",
    "                    # Handle the case when the CSV file is empty\n",
    "                    df_existing = pd.DataFrame()\n",
    "                \n",
    "                # Check if the existing DataFrame is empty\n",
    "                if df_existing.empty:\n",
    "                    # Save the new DataFrame to a new CSV file\n",
    "                    df_new.to_csv('tweets.csv', index=False)\n",
    "                    logging.info(\"New CSV file created with the downloaded tweets.\")\n",
    "                else:\n",
    "                    # Concatenate the existing and new data\n",
    "                    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "                    \n",
    "                    # Save the combined DataFrame to the CSV file\n",
    "                    df_combined.to_csv('tweets.csv', index=False)\n",
    "                    logging.info(\"Tweets appended to the existing CSV file.\")\n",
    "            else:\n",
    "                # Save the new DataFrame to a new CSV file\n",
    "                df_new.to_csv('tweets.csv', index=False)\n",
    "                logging.info(\"New CSV file created with the downloaded tweets.\")\n",
    "        else:\n",
    "            msg = \"No tweets found for the date range: {} to {}\"\n",
    "            logging.info(msg.format(start_date_str, end_date_str))\n",
    "            \n",
    "    except tweepy.TweepyException as e:\n",
    "        if e.api_code == 88:\n",
    "            # Rate limit reached, wait for the specified duration\n",
    "            wait_time = int(e.response.headers['Retry-After'])\n",
    "            msg = \"Rate limit reached. Sleeping for: {} seconds.\"\n",
    "            logging.info(msg.format(wait_time))\n",
    "            time.sleep(wait_time)\n",
    "        logging.error(\"Error: Failed to download tweets.\")\n",
    "        logging.error(e)\n",
    "    \n",
    "    # Update the start date for the next iteration\n",
    "    start_date = end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f23fea",
   "metadata": {},
   "source": [
    "# Log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10531093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the log file into a DataFrame\n",
    "log_df = pd.read_csv(log_file, sep=\":\", names=[\"Type\",\"User\",\"Log MSG\", \"Timestamp\"])\n",
    "\n",
    "# Define the CSV file name for saving the log DataFrame\n",
    "csv_file = f\"TwitterAPI_{today}.csv\"\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "if os.path.isfile(csv_file):\n",
    "    # Append the log DataFrame to the existing CSV file\n",
    "    log_df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "else:\n",
    "    # Save the log DataFrame to a new CSV file\n",
    "    log_df.to_csv(csv_file, index=False)\n",
    "\n",
    "# Display the log DataFrame\n",
    "log_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482b2ac4",
   "metadata": {},
   "source": [
    "# Cleaning tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c079f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the tweets data from the CSV file\n",
    "    df_tweets = pd.read_csv('tweets.csv')\n",
    "\n",
    "    # Function to clean a single tweet\n",
    "    def clean_tweet(tweet):\n",
    "        # Remove unnecessary characters and links\n",
    "        cleaned_tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "        cleaned_tweet = re.sub(r'http\\S+|www\\S+', '', cleaned_tweet)\n",
    "\n",
    "        # Remove Twitter usernames\n",
    "        cleaned_tweet = re.sub(r'@[^\\s]+', '', cleaned_tweet)\n",
    "\n",
    "        # Remove non-English words\n",
    "        cleaned_words = []\n",
    "        english_words = set(words.words())\n",
    "        for word in cleaned_tweet.split():\n",
    "            if word.lower() in english_words:\n",
    "                cleaned_words.append(word)\n",
    "        cleaned_tweet = ' '.join(cleaned_words)\n",
    "\n",
    "        return cleaned_tweet\n",
    "\n",
    "    # Clean the tweets column\n",
    "    df_tweets['Tweet'] = df_tweets['Tweet'].apply(clean_tweet)\n",
    "\n",
    "    # Remove rows with empty tweet values\n",
    "    df_tweets = df_tweets.dropna(subset=['Tweet'])\n",
    "\n",
    "    # Save the cleaned tweets back to the CSV file\n",
    "    df_tweets.to_csv('tweets.csv', index=False)\n",
    "\n",
    "    # Log the execution of the tweet cleaning process\n",
    "    logging.info(\"Tweet cleaning process completed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Log any exceptions or errors that occur\n",
    "    logging.error(\"Error occurred during tweet cleaning process.\")\n",
    "    logging.error(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fabffb7",
   "metadata": {},
   "source": [
    "# Tweet EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tweets data from the CSV file\n",
    "df_tweets = pd.read_csv('tweets.csv')\n",
    "# Perform basic exploratory data analysis (EDA)\n",
    "print(\"Number of tweets:\", len(df_tweets))\n",
    "print(\"Columns:\", df_tweets.columns)\n",
    "print(\"Sample tweets:\")\n",
    "print(df_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad86bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Perform sentiment analysis using Vader SentimentIntensityAnalyzer\n",
    "    nltk.download('vader_lexicon')\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    df_tweets['Sentiment'] = df_tweets['Tweet'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "    # Visualize sentiment distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(df_tweets['Sentiment'], bins=30, kde=True)\n",
    "    plt.title('Sentiment Distribution')\n",
    "    plt.xlabel('Sentiment Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # Log the successful execution of sentiment analysis and visualization\n",
    "    logging.info(\"Sentiment analysis and visualization completed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Log any exceptions or errors that occur\n",
    "    logging.error(\"Error occurred during sentiment analysis and visualization.\")\n",
    "    logging.error(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b8e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word cloud of most frequent words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordcloud = WordCloud(stopwords=stop_words, background_color='white').generate(' '.join(df_tweets['Tweet']))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Most Frequent Words in Tweets')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b9608",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e34aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the tweets from the CSV file\n",
    "    df = pd.read_csv('tweets.csv')\n",
    "\n",
    "    # Perform sentiment analysis using TextBlob\n",
    "    df['sentiment'] = df['Tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "    # Classify sentiment as positive, negative, or neutral\n",
    "    df['sentiment_label'] = df['sentiment'].apply(lambda x: 'Positive' if x > 0 else 'Negative' if x < 0 else 'Neutral')\n",
    "\n",
    "    # Save the updated DataFrame to CSV\n",
    "    df.to_csv('tweets_sentiment.csv', index=False)\n",
    "\n",
    "except pd.errors.EmptyDataError:\n",
    "    # Handle the case when the CSV file is empty\n",
    "    logging.error(\"Error: The CSV file is empty.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # Handle the case when the CSV file is not found\n",
    "    logging.error(\"Error: The CSV file 'tweets.csv' does not exist.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle any other exceptions that may occur\n",
    "    logging.error(\"An error occurred during sentiment analysis and classification.\")\n",
    "    logging.error(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f32f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert forecast data to strings\n",
    "forecast_1w_str = forecast_1w.to_string(header=False)\n",
    "forecast_1m_str = forecast_1m.to_string(header=False)\n",
    "forecast_3m_str = forecast_3m.to_string(header=False)\n",
    "# Print the forecast data\n",
    "print(\"1 Week Forecast:\")\n",
    "print(forecast_1w_str)\n",
    "print(\"1 Month Forecast:\")\n",
    "print(forecast_1m_str)\n",
    "print(\"3 Months Forecast:\")\n",
    "print(forecast_3m_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d34df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tweets data from the CSV file\n",
    "df_tweets = pd.read_csv('tweets_sentiment.csv')\n",
    "\n",
    "# Perform basic exploratory data analysis (EDA)\n",
    "print(\"Number of tweets:\", len(df_tweets))\n",
    "print(\"Columns:\", df_tweets.columns)\n",
    "print(\"Sample tweets:\")\n",
    "print(df_tweets.head())\n",
    "\n",
    "# Visualize sentiment distribution by sentiment category\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Sentiment', hue='Category', data=df_tweets)\n",
    "plt.title('Sentiment Distribution by Category')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffc738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Sentiment', data=df_tweets)\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a1fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' column to datetime\n",
    "df_tweets['Date'] = pd.to_datetime(df_tweets['Date'])\n",
    "\n",
    "# Group by date and sentiment to calculate counts\n",
    "df_sentiment_counts = df_tweets.groupby(['Date', 'Sentiment']).size().reset_index(name='Count')\n",
    "\n",
    "# Pivot the data to have sentiment types as columns\n",
    "df_sentiment_pivot = df_sentiment_counts.pivot(index='Date', columns='Sentiment', values='Count')\n",
    "\n",
    "# Plot the sentiment distribution over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=df_sentiment_pivot, dashes=False)\n",
    "plt.title('Sentiment Distribution Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Sentiment')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372696a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the tweets into words\n",
    "tokenized_words = [word.lower() for tweet in df_tweets['Tweet'] for word in word_tokenize(tweet)]\n",
    "\n",
    "# Calculate the frequency distribution of words\n",
    "freq_dist = FreqDist(tokenized_words)\n",
    "most_common = freq_dist.most_common(20)\n",
    "\n",
    "# Plot the most frequent words by sentiment\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Count', y='Word', hue='Sentiment', data=pd.DataFrame(most_common, columns=['Word', 'Count']))\n",
    "plt.title('Most Frequent Words by Sentiment')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Word')\n",
    "plt.legend(title='Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ad8d6",
   "metadata": {},
   "source": [
    "# Time series forecast of the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3693114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tweet sentiment data from the CSV file\n",
    "df = pd.read_csv('tweets_sentiment.csv', parse_dates=['Date'])\n",
    "\n",
    "# Set the 'Date' column as the index\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "try:\n",
    "    # Fit an ARIMA model to the sentiment data\n",
    "    model = sm.tsa.ARIMA(df['sentiment'], order=(1, 0, 1), trend='c').fit()\n",
    "\n",
    "    # Generate predictions for the next 1 week, 1 month, and 3 months\n",
    "    forecast_1w = model.predict(start=len(df), end=len(df) + 6, dynamic=False)\n",
    "    forecast_1m = model.predict(start=len(df), end=len(df) + 30, dynamic=False)\n",
    "    forecast_3m = model.predict(start=len(df), end=len(df) + 90, dynamic=False)\n",
    "\n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add actual sentiment data\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['sentiment'], name='Actual'))\n",
    "\n",
    "    # Add forecasted sentiment data\n",
    "    forecast_dates_1w = pd.date_range(start=df.index[-1], periods=7)[1:]\n",
    "    forecast_dates_1m = pd.date_range(start=df.index[-1], periods=31)[1:]\n",
    "    forecast_dates_3m = pd.date_range(start=df.index[-1], periods=91)[1:]\n",
    "    fig.add_trace(go.Scatter(x=forecast_dates_1w, y=forecast_1w, name='1 Week Forecast'))\n",
    "    fig.add_trace(go.Scatter(x=forecast_dates_1m, y=forecast_1m, name='1 Month Forecast'))\n",
    "    fig.add_trace(go.Scatter(x=forecast_dates_3m, y=forecast_3m, name='3 Months Forecast'))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Time Series Forecast of Sentiment',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Sentiment',\n",
    "        legend_title='Forecast',\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "\n",
    "    # Show the interactive Plotly graph\n",
    "    fig.show()\n",
    "\n",
    "except ValueError as e:\n",
    "    print(\"Error: Failed to make time series forecast.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67000bb6",
   "metadata": {},
   "source": [
    "# Pyspark Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416dd4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the values below with yours\n",
    "ACCESS_TOKEN = access_token\n",
    "ACCESS_SECRET = access_token_secret\n",
    "CONSUMER_KEY = consumer_key\n",
    "CONSUMER_SECRET = consumer_secret\n",
    "my_auth = requests_oauthlib.OAuth1(CONSUMER_KEY, CONSUMER_SECRET,ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "def get_tweets():\n",
    "    url = 'https://stream.twitter.com/1.1/statuses/filter.json'\n",
    "    query_data = [('language', 'en'), ('locations', '-130,-20,100,50'),('track','#')]\n",
    "    query_url = url + '?' + '&'.join([str(t[0]) + '=' + str(t[1]) for t in query_data])\n",
    "    response = requests.get(query_url, auth=my_auth, stream=True)\n",
    "    print(query_url, response)\n",
    "    return response\n",
    "\n",
    "def send_tweets_to_spark(http_resp, tcp_connection):\n",
    "    for line in http_resp.iter_lines():\n",
    "        try:\n",
    "            full_tweet = json.loads(line)\n",
    "            tweet_text = full_tweet['text']\n",
    "            print(\"Tweet Text: \" + tweet_text)\n",
    "            print (\"------------------------------------------\")\n",
    "            tcp_connection.send(tweet_text + '\\n')\n",
    "        except:\n",
    "            e = sys.exc_info()[0]\n",
    "            print(\"Error: %s\" % e)\n",
    "            \n",
    "TCP_IP = \"localhost\"\n",
    "TCP_PORT = 9009\n",
    "conn = None\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "s.bind((TCP_IP, TCP_PORT))\n",
    "s.listen(1)\n",
    "print(\"Waiting for TCP connection...\")\n",
    "conn, addr = s.accept()\n",
    "print(\"Connected... Starting getting tweets.\")\n",
    "resp = get_tweets()\n",
    "send_tweets_to_spark(resp, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbc5576",
   "metadata": {},
   "source": [
    "# Configure Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aaeb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark configuration\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"TwitterStreamApp\")\n",
    "# create spark context with the above configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "# create the Streaming Context from the above spark context with interval size 2 seconds\n",
    "ssc = StreamingContext(sc, 2)\n",
    "# setting a checkpoint to allow RDD recovery\n",
    "ssc.checkpoint(\"checkpoint_TwitterApp\")\n",
    "# read data from port 9009\n",
    "dataStream = ssc.socketTextStream(\"localhost\",9009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec47581",
   "metadata": {},
   "source": [
    "# Filter Hastags and stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each tweet into words\n",
    "words = dataStream.flatMap(lambda line: line.split(\" \"))\n",
    "# filter the words to get only hashtags, then map each hashtag to be a pair of (hashtag,1)\n",
    "hashtags = words.filter(lambda w: '#' in w).map(lambda x: (x, 1))\n",
    "# adding the count of each hashtag to its last count\n",
    "tags_totals = hashtags.updateStateByKey(aggregate_tags_count)\n",
    "# do processing for each RDD generated in each interval\n",
    "tags_totals.foreachRDD(process_rdd)\n",
    "# start the streaming computation\n",
    "ssc.start()\n",
    "# wait for the streaming to finish\n",
    "ssc.awaitTermination()\n",
    "\n",
    "def aggregate_tags_count(new_values, total_sum):\n",
    "    return sum(new_values) + (total_sum or 0)\n",
    "\n",
    "def get_sql_context_instance(spark_context):\n",
    "    if ('sqlContextSingletonInstance' not in globals()):\n",
    "        globals()['sqlContextSingletonInstance'] = SQLContext(spark_context)\n",
    "    return globals()['sqlContextSingletonInstance']\n",
    "def process_rdd(time, rdd):\n",
    "    print(\"----------- %s -----------\" % str(time))\n",
    "    try:\n",
    "        # Get spark sql singleton context from the current context\n",
    "        sql_context = get_sql_context_instance(rdd.context)\n",
    "        # convert the RDD to Row RDD\n",
    "        row_rdd = rdd.map(lambda w: Row(hashtag=w[0], hashtag_count=w[1]))\n",
    "        # create a DF from the Row RDD\n",
    "        hashtags_df = sql_context.createDataFrame(row_rdd)\n",
    "        # Register the dataframe as table\n",
    "        hashtags_df.registerTempTable(\"hashtags\")\n",
    "        # get the top 10 hashtags from the table using SQL and print them\n",
    "        hashtag_counts_df = sql_context.sql(\"select hashtag, hashtag_count from hashtags order by hashtag_count desc limit 10\")\n",
    "        hashtag_counts_df.show()\n",
    "        # call this method to prepare top 10 hashtags DF and send them\n",
    "        send_df_to_dashboard(hashtag_counts_df)\n",
    "    except:\n",
    "        e = sys.exc_info()[0]\n",
    "        print(\"Error: %s\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5743eaf",
   "metadata": {},
   "source": [
    "# Stream to dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec88cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_df_to_dashboard(df):\n",
    "    # extract the hashtags from dataframe and convert them into array\n",
    "    top_tags = [str(t.hashtag) for t in df.select(\"hashtag\").collect()]\n",
    "    # extract the counts from dataframe and convert them into array\n",
    "    tags_count = [p.hashtag_count for p in df.select(\"hashtag_count\").collect()]\n",
    "    # initialize and send the data through REST API\n",
    "    url = 'http://localhost:5001/updateData'\n",
    "    request_data = {'label': str(top_tags), 'data': str(tags_count)}\n",
    "    response = requests.post(url, data=request_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
